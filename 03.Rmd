```{r, include=FALSE}
knitr::opts_chunk$set(
  echo=TRUE, message=FALSE, warning=FALSE, fig.align="center", dev="svglite"
)

old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(crayon.enabled=TRUE)
```

# Lab 5 &mdash; October 13

## Packages

```{r}
library(tibble)
library(dplyr)
library(ggplot2)
library(tidyr)
library(broom)
library(ellipse)
library(palmerpenguins)
theme_set(theme_bw())
```

The [tidyr](https://tidyr.tidyverse.org/) package is another component package of the
[tidyverse](https://www.tidyverse.org/packages/) that contains functions for cleaning up and
reshaping data.

The [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) package will be used for its
`penguins` data set for the multiple linear regression example today.

We will be using the package [ellipse](https://CRAN.R-project.org/package=ellipse) for drawing
ellipses (confidence regions). This package contains functions for drawing confidence regions
associated to the estimates obtained from linear regression. Unlike the code provided in class for
drawing confidence regions with the component confidence intervals, which took in raw data as input,
the `ellipse::ellipse.lm()` function will allow you to pass in your linear model object instead. In
using the [ellipse](https://CRAN.R-project.org/package=ellipse) package, we will however need to
draw the lines corresponding to the component confidence intervals ourselves (though this is not
difficult to do).


## The pipe

The pipe operator, `%>%`, can be found in the package [magrittr](https://magrittr.tidyverse.org/),
which should have been installed when you installed the [dplyr](https://dplyr.tidyverse.org/)
package. When [dplyr](https://dplyr.tidyverse.org/) is loaded (by calling `library(dplyr)`),
the [magrittr](https://magrittr.tidyverse.org/) pipe is also imported.

The pipe works by taking the result on the left and passing it to the **first** argument of the
function on the right. The pipe allows you to write cleaner code by converting something like this:

```{r, eval=FALSE}
mutate(select(filter(mydata, var1 > 5), var2, var3), var4 = var2 + sqrt(var3))
```

into this:

```{r, eval=FALSE}
mydata %>%
  filter(var1 > 5) %>%
  select(var2, var3) %>%
  mutate(var4 = var2 + sqrt(var3))
```

revealing the underlying sequential logic of your workflow. Since the pipe works by taking the
result on the left and passing it to the **first** argument of the function on the right, the use of
the pipe when wrangling data with [dplyr](https://dplyr.tidyverse.org/) is especially convenient as
all [dplyr](https://dplyr.tidyverse.org/) data wrangling functions have the data frame argument as
its **first** argument, and in most cases, will return a data frame.

In the example above, the first argument to `filter()`, `select()`, and `mutate()` is a data frame.
After evaluation, each function also returns a data frame. Notice that as a result of piping a data
frame into each of these functions, we don't need to actually specify where it's going (since it
goes to the first argument by default) and we don't need to create extra variables for intermediate
steps.


## Multiple linear regression &mdash; hypothesis testing

### The penguins data

Let us load the `penguins` data set. Some of the variables that we are interested in working with
contain missing values. Immediately after loading the data, we can pass it into the `drop_na()`
function (from [tidyr](https://tidyr.tidyverse.org/)) which will drop all rows (observations) that
contain missing values. 

In addition, let's also rename some of our variables to shorten some of the code that we need to
type. This can be done by piping our data into the `rename()` function and supplying name pairs
in the format `new_name = old_name`.

```{r}
penguins <- palmerpenguins::penguins %>%
  drop_na() %>%
  rename(
    bill_length = bill_length_mm,
    bill_depth = bill_depth_mm,
    flipper_length = flipper_length_mm,
    body_mass = body_mass_g
  )

penguins
```


### Subsetting the data

This data set contains penguins from three species across three islands:

```{r}
penguins %>%
  distinct(species, island)
```

For the following example, let us focus only on the penguins of species *Adelie* from the island
*Biscoe*. We can obtain this subset of the `penguins` data using the following code:

```{r}
adeliebiscoe <- penguins %>%
  filter(species == "Adelie", island == "Biscoe")
```

*(Note that in the code above, we are using two equal signs since we are making a comparison and
not setting values).*


### Visual check for linear relationship

Suppose we are interested in fitting a model using body mass as the response and bill length, bill
depth, and flipper length as the predictors. Before building the model, we should verify that there
is a linear relationship between the response and the individual predictors.

```{r}
ggplot(adeliebiscoe, aes(x=bill_length, y=body_mass))+
  geom_point()
```

```{r}
ggplot(adeliebiscoe, aes(x=bill_depth, y=body_mass))+
  geom_point()
```

```{r}
ggplot(adeliebiscoe, aes(x=flipper_length, y=body_mass))+
  geom_point()
```

The plots look okay. Let's go ahead and fit the model.


### Fit the model

```{r}
ad_lm <- lm(body_mass ~ bill_length + bill_depth + flipper_length, data=adeliebiscoe)

summary(ad_lm)
```

From the above output, the equation of our fitted line is:

\[\widehat{y}_{i} \,=\, -6122 \,+\, 70.743x_{1,\,i} \,+\, 165.196x_{2,\,i} + 21.397x_{3,\,i}\]

where:

- $\widehat{y}_{i}$ is the predicted body mass
- $x_{1,\,i}$ is the bill length
- $x_{2,\,i}$ is the bill depth
- $x_{3,\,i}$ is the flipper length


### Fitting a model without an intercept

By default, linear models are fitted with an intercept. We can fit a linear model without an
intercept by adding a `+ 0` or a `- 1` in the formula specification (see the Details section of
`?lm`). To make the hypothesis tests that we will do in the next section a bit more interesting,
let us remove the intercept from the model that we just fit and overwrite it.

As shown in Lab 1, we can do this by fitting a brand new model using `lm()`:

```{r, eval=FALSE}
ad_lm <- lm(body_mass ~ bill_length + bill_depth + flipper_length - 1, data=adeliebiscoe)
```

or by using `update()`:

```{r}
ad_lm <- update(ad_lm, formula = . ~ . - 1)
```

Recall that the above line of code means that (the new) `ad_lm` is constructed by taking the
existing `ad_lm` and updating its formula. The response variable remains the same (represented by
the dot on the left of the tilde), the predictors remain the same (represented by the dot on the
right of the tilde) and a `- 1` denotes that we do not want an intercept.

Let's take a look at our new model.

```{r}
summary(ad_lm)
```

Interestingly, by removing the intercept from the model, our adjusted R-squared value went from
0.6246 to 0.9905 &#x1F440;.


### ANOVA with multiple linear regression models

```{r}
anova(ad_lm)
```

For multiple linear regression, the output resulting from passing our linear model to `anova()` is
slightly different from what we saw with simple linear regression. In simple linear regression,
wrapping our linear model with `anova()` produced the "classic" $F$-table: the sum of squares for
the first row was the SSR, the sum of squares for the second row was the SSE, and there was a single
$F$-value and corresponding $p$-value.

In the above output, we now have three $F$-values and three corresponding $p$-values! However, the
interpretation here is slightly different. The sum of squares column represents the **sequential**
increase in SSR (or decrease in SSE) by adding the variable to the model that came before it. As
such:

- The sum of squares on the `bill_length` row is the increase in SSR (or decrease in SSE) by
including only `bill_length` in the model (because our base model has no intercept)
- The sum of squares on the `bill_depth` row is the increase in SSR (or decrease in SSE) by
including `bill_depth` in the model that **already** includes `bill_length`
- The sum of squares on the `flipper_length` row is the increase in SSR (or decrease in SSE) by
including `flipper_length` to the model that **already** includes `bill_length` and `bill_depth`
- The sum of squares on the `Residual` row is the SSE of the model that includes **all** of
`bill_length`, `bill_depth`, and `flipper_length`

Using only the output above, if we wish to perform a hypothesis test to check whether a subset of
the parameters are different from zero, we need to do some extra math. But since we are using R,
let's not do math! We can perform the partial $F$-tests that we are interested in by constructing
reduced models (and this is where the `update()` function becomes extremely handy).


### At least one parameter non-zero

Suppose we are interested in testing the hypotheses:

\[H_{0}: \beta_{1} \,=\, \beta_{2} \,=\, \beta_{3} \,=\, 0, \quad
H_{A}: \text{At least one non-zero}\]

The reduced model is the intercept-only model that passes through the origin.

```{r}
ad_lm_origin_only <- update(ad_lm, formula = . ~ 0)
```

The full model is the model that has all the predictors (`ad_lm`).

To test the above hypotheses, we pass our reduced and full models to `anova()`:

```{r}
anova(ad_lm_origin_only, ad_lm)
```

The produces a $F$-value of 1535.8 with numerator degrees of freedom, 3, and denominator degrees of
freedom, 41. But we don't need to use these values since a $p$-value is also given. Using a
significance level of $\alpha = 0.05$, since the $p$-value is less than 0.05, we reject the null
hypothesis and conclude that at least one of the parameters is non-zero and as such, we should not
consider a constant $(Y \,=\, 0)$ model. It is important to note that the result of this hypothesis
test tells us that at least one parameter is non-zero, but it doesn't tell us **which**.

A hypothesis test that has a null hypothesis where all parameters are equal to zero is often known
as a test for model usefulness. If we fail to reject this null hypothesis, it suggests that the
current model is not a substantial improvement over a constant $(Y \,=\, c)$ model.


### Two parameters non-zero

Suppose we are interested in testing the hypotheses:

\[H_{0}: \beta_{2} \,=\, \beta_{3} \,=\, 0, \quad H_{A}: \text{At least one non-zero}\]

The reduced model is the model that contains only `bill_length`. We can construct this model using
`update()` either by specifying the variables we want:

```{r}
# Don't forget the no-intercept term!!
ad_lm1 <- update(ad_lm, formula = . ~ bill_length - 1)
```

or the variables we do not want

```{r, eval=FALSE}
ad_lm1 <- update(ad_lm, formula = . ~ . - bill_depth - flipper_length)
```

To test the above hypotheses, we pass our reduced and full models to `anova()`:

```{r}
anova(ad_lm1, ad_lm)
```

This produces a $F$-value of 2.9407 with numerator degrees of freedom, 2, and denominator degrees
of freedom, 41. Using the usual significance level of $\alpha = 0.05$, since the $p$-value is
greater than 0.05, we fail to reject the null hypothesis and conclude that there is insufficient
evidence to support the claim that at least one of $\beta_{2}$ or $\beta_{3}$ are non-zero.


### One parameter non-zero

Suppose we are interested in testing the hypotheses:

\[H_{0}: \beta_{3} \,=\, 0, \quad H_{A}: \beta_{3} \,\neq\, 0\]

This hypothesis can be carried out by performing a $F$-test or a $t$-test (the $t$-test is much
simpler). If we want to perform a $F$-test, we take the usual steps of first constructing the
reduced model. The reduced model is the model that contains `bill_length` and `bill_depth`. We can
construct this model by using `update()` and removing `flipper_length`.

```{r}
ad_lm12 <- update(ad_lm, formula = . ~ . - flipper_length)
```

To test the above hypotheses, we pass our reduced and full models to `anova()`:

```{r}
anova(ad_lm12, ad_lm)
```

This produces a $F$-value of 0.7177 with numerator degrees of freedom, 1, and denominator degrees
of freedom, 41. Using the usual significance level of $\alpha = 0.05$, since the $p$-value is
greater than 0.05, we fail to reject the null hypothesis and conclude that there is insufficient
evidence to support the claim that $\beta_{3}$ is different from zero.

In testing a single parameter non-zero, we could have done a $t$-test, whose corresponding $p$-value
could have been read off the coefficient summary table.

```{r}
tidy(ad_lm)
```

The corresponding $p$-value for this $t$-test is 0.402. Since this $p$-value is greater than 0.05,
we fail to reject the null hypothesis once again.


### Caution &#x2757;

Suppose we had output that looked like:

```{r, echo=FALSE}
tibble(
  term = c("var1", "var2", "var3"),
  estimate = "...",
  std.error = "...",
  statistic = "...",
  p.value = c(0.0001, 0.4971, 0.8846)
)
```

It is **not** correct to say:

> The $p$-values for the tests of `var2` different from zero and `var3` different from zero are
0.4971 and 0.8846, respectively. Since both $p$-values are larger than 0.05, we fail to reject the
null hypothesis in both tests. Therefore we can simultaneously remove `var2` and `var3` from our
model.

The $p$-value of 0.497 corresponds to testing `var2` different from zero, assuming `var1` and `var3`
are included in model. Similarly, the $p$-value of 0.885 corresponds to testing `var3` different
from zero, assuming that `var1` and `var2` are included in the model. Therefore the proper procedure
using $t$-tests would be to test for a single parameter being non-zero, remove the variable from the
model (assuming we failed to reject the null hypothesis), refit a new model without this variable,
and then perform a second $t$-test.


## Multiple linear regression &mdash; working with matrices in R

### Creating a matrix in R

To begin, I recommend checking out the related documentation by calling `?matrix`. The main
arguments of interest are:

- `data`: the data for your matrix
- `nrow`: the number of rows your matrix should have
- `ncol`: the number of columns your matrix should have
- `byrow`: when set to `TRUE`, the supplied data are filled in horizontally (by row), otherwise the
data are filled in vertically (by column). By default data is filled in vertically.

To illustrate how the data gets filled in depending on the value of `byrow`:

```{r}
nums <- 1:9

mat1 <- matrix(nums, nrow=3, ncol=3)
mat1
```

```{r}
nums2 <- c(1:4, 9, 2, 7, 3, 6)
mat2 <- matrix(nums2, nrow=3, ncol=3, byrow=TRUE)
mat2
```

### Matrix operations

#### Matrix transpose

To obtain the transpose of a matrix, use `t()`:

```{r}
mat1
t(mat1)
```


#### Matrix inverse

To obtain the inverse of a matrix, use `solve()`:

```{r}
mat2
solve(mat2)
```

`solve()` will return an error if your matrix is not invertible.


#### Matrix multiplication

To multiply two matrices together, use `%*%`:

```{r}
mat1 %*% mat2
```

You **cannot** use the usual multiplication operator because that will perform element-wise
multiplication, rather than matrix multiplication:

```{r}
mat1
mat2
mat1 * mat2
```


### Example: Question K, page 174 (selected parts)

From the given data, we can start by constructing our matrices.

```{r}
Y <- matrix(c(7.2, 8.1, 9.8, 12.3, 12.9), nrow=5, ncol=1)
Y
```

Don't forget the column of ones in the design matrix that corresponds to the intercept! Let's also
supply some column names to the design matrix to keep track of the columns and so that our
resulting coefficient estimates will have names. 

```{r}
X <- matrix(
  c(rep(1, 5), -1, -1, 0, 1, 1, -1, 0, 0, 0, 1),
  nrow=5, ncol=3,
  dimnames = list(NULL, c("Intercept", "X1", "X2"))
)
X
```

In supplying names to a matrix during its construction, a list of length 2 must be supplied
specifying the row and column names respectively. Since I don't need any rownames I supply `NULL` to
the first element of the list. Note that this list has length 2: `NULL` is one element, and 
`c("Intercept", "X1", "X2")` is another element. An alternative is to build a matrix with no names
and set the names afterward using:

```{r, eval=FALSE}
rownames(X) <- c("row1", "row2", "etc.")
colnames(X) <- c("col1", "col2", "etc.")
```


#### Obtain the coefficient estimates

We can obtain the coefficient estimates using the equation:

\[\widehat{\beta} \,=\, (X^{T}X)^{-1}\,X^{T}\,Y\]

```{r}
betahat <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat
```

The equation of the fitted line is:

\[\widehat{y}_{i} \,=\, 10.06 \,+\, 2.10x_{1,\,i} \,+\, 0.75x_{2,\,i}\]


#### Compute the following regression sum of squares

##### (i)

\[SS(\widehat{\beta}_{0},\, \widehat{\beta}_{1},\, \widehat{\beta}_{2}) \,=\, \widehat{\beta}^{T}\,X^{T}\,Y\]

```{r}
t(betahat) %*% t(X) %*% Y
```


##### (ii)
\[SS(\widehat{\beta}_{1},\, \widehat{\beta}_{2} \,|\, \widehat{\beta}_{0}) \,=\, \widehat{\beta}^{T}\,X^{T}\,Y \,-\, \frac{1}{n}Y^{T}\,Y\]

```{r}
(t(betahat) %*% t(X) %*% Y) - (1/5)*(t(Y) %*% Y)
```


##### (iii)

\[SS(\widehat{\beta}_{2} \,|\, \widehat{\beta}_{1},\, \widehat{\beta}_{0}) \,=\, SS(\widehat{\beta}_{0},\, \widehat{\beta}_{1},\, \widehat{\beta}_{2})
\,-\, SS(\widehat{\beta}_{0},\,\widehat{\beta}_{1})\]

We already have the first term. The second term is obtained by fitting a model without the $X_{2}$
variable and computing

\[SS(\widehat{\beta}_{0},\, \widehat{\beta}_{1}) \,=\, \widehat{\beta}^{T}\,X^{T}\,Y\]

```{r}
X_reduced <- X[, -3]
X_reduced

betahat_reduced <- solve(t(X_reduced) %*% X_reduced) %*% t(X_reduced) %*% Y
betahat_reduced

t(betahat_reduced) %*% t(X_reduced) %*% Y
```

Therefore the quantity

\[SS(\widehat{\beta}_{2} \,|\, \widehat{\beta}_{1},\, \widehat{\beta}_{0}) \,=\, SS(\widehat{\beta}_{0},\, \widehat{\beta}_{1},\,\widehat{\beta}_{2})
\,-\, SS(\widehat{\beta}_{0},\,\widehat{\beta}_{1})$\]

can be computed as:

```{r}
(t(betahat) %*% t(X) %*% Y) - (t(betahat_reduced) %*% t(X_reduced) %*% Y)
```


#### Obtain the residual sum of squares

\[SSE \,=\, Y^{T}\,Y \,-\, \widehat{\beta}^{T}\,X^{T}\,Y\]

```{r}
SSE <- (t(Y) %*% Y) - (t(betahat) %*% t(X) %*% Y)
SSE
```


#### Obtain an estimate for error term variance

\[\widehat{\sigma}^{2} \,=\, S^{2} \,=\, MSE \,=\, \frac{SSE}{\text{df}_{SSE}}\]

The degrees of freedom of the SSE is $n - p - 1$ where $n$ is the number of observations, and $p$
is the number of non-intercept parameters estimated. Here, $n = 5$ and $p = 2$. So

\[\text{df}_{SSE} \,=\, n - p - 1 \,=\, 5 - 2 - 1 \,=\, 2\]

```{r}
MSE <- SSE / 2
MSE <- as.numeric(MSE)
MSE
```

Note: We need to wrap `MSE` in `as.numeric()` to coerce it to a scalar for future calculations.
Otherwise, in R, this value is still treated as a 1x1 matrix and matrix operations may fail due
to incompatible matrix dimensions.


#### Obtain the variance-covariance matrix of the coefficient estimates

\[\textbf{Var}(\widehat{\beta}) \,=\, \widehat{\sigma}^{2}(X^{T}\,X)^{-1}\]

```{r}
vcov_betahat <- MSE * solve(t(X) %*% X)
vcov_betahat
```

The variances are along the diagonal of the matrix. The covariances are the off-diagonal terms. If
we wanted the standard errors for all coefficient estimates we would need to take the square root
of all the diagonal terms. We can do this by first extracting the diagonal elements by wrapping
`vcov_betahat` with `diag()`, and then wrapping it in `sqrt()`.

```{r}
se_betahat <- sqrt(diag(vcov_betahat))
se_betahat
```


#### Predict the mean response value at a given point

Find $\widehat{Y}_{0}$ at point $(X_{1,\,0},\, X_{2,\,0}) \,=\, (0.5, 0)$.

\[\widehat{y}_{0} \,=\, x_{0}^{T}\widehat{\beta} \,=\, \begin{bmatrix}1 &0.5 &0\end{bmatrix}
\begin{bmatrix}10.06\\ 2.10\\ 0.75\end{bmatrix}\]

```{r}
x0 <- matrix(c(1, 0.5, 0), nrow=3, ncol=1)
x0

t(x0) %*% betahat
```


#### Obtain the standard error of the mean response at the given point

\[\textbf{se}(\widehat{Y}_{0}) \,=\, \sqrt{\widehat{\sigma}^{2}x_{0}^{T}(X^{T}\,X)^{-1}x_{0}}\]

```{r}
sqrt(MSE * t(x0) %*% solve(t(X) %*% X) %*% x0)
```


#### Weighted least squares

From the information given in the question, the variance of the error term is now

\[\textbf{Var}(\varepsilon) \,=\, \sigma^{2}V\]

where

\[V \,=\, 
\begin{bmatrix}
1 &0 &0 &0 &0\\
0 &1 &0 &0 &0\\
0 &0 &1/4 &0 &0\\
0 &0 &0 &1 &0\\
0 &0 &0 &0 &1
\end{bmatrix}\]

and $V^{-1}$ is the "weight matrix".

The new estimates are computed using:

\[\widehat{\beta} \,=\, (X^{T}\,V^{-1}\,X)^{-1}\,X^{T}\,V^{-1}\,Y\]

```{r}
V <- diag(c(1, 1, 1/4, 1, 1))
V
```

Recall that the inverse of a diagonal matrix is still a diagonal matrix where all the diagonal terms
have been reciprocated.

```{r}
V_inv <- solve(V)
V_inv
```

```{r}
betahat_weighted <- solve(t(X) %*% V_inv %*% X) %*% t(X) %*% V_inv %*% Y
betahat_weighted
```


## Simple linear regression &mdash; simultaneous confidence regions

Let us return to the `rock` data set and consider the simple linear regression model of `area` as
the response variable and `peri` as the predictor.

```{r}
data(rock)

slr_rock <- lm(area ~ peri, data=rock)
```

Suppose we are interested in constructing a 90% confidence region for $(\beta_{0}, \beta_{1})$ with
individual 95% confidence intervals. We can start by obtaining the path of this ellipse by passing
our linear model to `ellipse()`. The default confidence level for the ellipse is 95% so we need to
make a slight adjustment to the value of the `level` parameter. In addition, `ellipse()` returns a
matrix of coordinates for the path of the ellipse. Since we'll be passing the coordinates of the
path of the ellipse to `ggplot` later, we should also convert it to a data frame.

```{r}
ellipse_path <- slr_rock %>%
  ellipse(level=0.90) %>%
  as_tibble()

ellipse_path
```

The values under the `(Intercept)` and `peri` columns represent the x- and y-values, respectively,
of the path of the ellipse. Note that `ellipse()` is another example of a generic function. When
passing a linear model object (an object of class `lm`) to `ellipse()`, it calls the more specific
`ellipse.lm()`.

Now that we have the path of the ellipse, we also need to obtain the point 
$(\widehat{\beta}_{0},\, \widehat{\beta}_{1})$, and the individual 95% confidence intervals for
$\beta_{0}$ and $\beta_{1}$. We can obtain these by passing our linear model to `tidy.lm()` and
specifying `conf.int=TRUE` to obtain both the coefficient table and corresponding 95% confidence
intervals.

```{r}
coef_table <- slr_rock %>%
  tidy(conf.int=TRUE)

coef_table
```

Before we can begin plotting, we have to do some data reshaping. Recall that when plotting with
`ggplot()`, we need all of our x-values in one column and all our y-values in another column.

We need a data frame where the estimate of $\beta_{0}$ is in one column and the estimate of
$\beta_{1}$ is in another column.

```{r}
coef_point <- coef_table %>%
  pivot_wider(id_cols=c(term, estimate), names_from=term, values_from=estimate)

coef_point
```

We need a data frame where the lower and upper bounds of the confidence interval for $\beta_{0}$
are in a single column.

```{r}
beta0 <- coef_table %>%
  slice(1) %>%
  pivot_longer(cols=contains("conf"))

beta0
```

We need a data frame where the lower and upper bounds of the confidence interval for $\beta_{1}$
are in a single column.

```{r}
beta1 <- coef_table %>%
  slice(2) %>%
  pivot_longer(cols=contains("conf"))

beta1
```

Now that our data is "in shape", we can start building the plot. This plot will be constructed by
taking advantage of the fact that every `geom_*()` layer has its own `data` argument. This means
that we can use a different data set for each layer of our plot. Since we don't have a common data
set or common aesthetics that will be shared by all layers, we will not supply anything to the call
to `ggplot()`. Try running the code below, layer by layer, to see how the plot is being constructed!

```{r}
ggplot()+
  geom_vline(data=beta0, aes(xintercept=value), colour="#3366FF", lty=2)+
  geom_hline(data=beta1, aes(yintercept=value), colour="#3366FF", lty=2)+
  geom_point(data=coef_point, aes(x=`(Intercept)`, y=peri), colour="red")+
  geom_path(data=ellipse_path, aes(x=`(Intercept)`, y=peri))+
  labs(
    x="Intercept", y="Slope",
    caption="Solid line represents the boundary of the 90% confidence region. Dashed lines represent
    the boundaries of the 95% confidence intervals."
  )
```

Note that points contained within the ellipse are values $(\beta_{0},\,\beta_{1})$ that the data
suggest are **jointly** reasonable for the parameters. Meanwhile, within the rectangular region
but outside of the ellipse are plausible values for the parameters when considered individually.

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo=TRUE, message=FALSE, warning=FALSE, fig.align="center", dev="svglite"
)

old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(crayon.enabled=TRUE)
```

# Lab 7 &mdash; November 10

## Packages

```{r}
library(tibble)
library(dplyr)
library(ggplot2)
library(tidyr)
library(readr)
library(broom)
library(leaps)
theme_set(theme_bw())
```

[readr](https://readr.tidyverse.org/) is another package from
[tidyverse](https://tidyverse.org/packages) for reading in various types of data. Many of its
functions work similar to the base-R equivalents but work a bit better, are smarter, and read data
in as a tibble.

We will be using the [leaps](https://cran.r-project.org/package=leaps) package to aid us in
computing all possible models from a set of predictors and obtaining the corresponding Mallows'
$C_{p}$ values for all of these models. The [leaps](https://cran.r-project.org/package=leaps)
package can also be used for best subset selection, forward selection, and backward selection. In
this lab, while we will be covering an example on backward selection, we will be doing it manually
to throroughly illustrate the process.


## Weighted linear regression (via `lm`)

Last time, we saw how to compute the weighted least squares estimates for $\beta$ using matrices.
Now, let's take a look at how to do it via the `lm()` function. The data we will use for this
example is the data from Table 2.1 (page 51).

I entered the data manually myself, so let's double-check that my values are correct. The resulting
regression equation should be $\widehat{Y}_{i} = 1.426 + 0.316X_{i}$ (page 51).

```{r}
table2.1 <- read.table("./data/table2.1_DS.txt", header=TRUE) %>%
  as_tibble()

lm(Y ~ X, data=table2.1) %>%
  coef()
```

Looks good!


### Problem (i)

Suppose we are told that the 16th observation has variance $16\sigma^{2}$ rather than $\sigma^{2}$.
Obtain the parameter estimates using weighted least squares.


### Solution

If the 16th observation has variance $16\sigma^{2}$, then

\[\textbf{Var}(\varepsilon) \,=\, \sigma^{2}V \,=\,
\sigma^{2}\begin{bmatrix}1\\
&1\\
& &\ddots\\
& & &16\\
& & & &\ddots\\
& & & & &1\\
& & & & & &1
\end{bmatrix}\]

The weight matrix, $V^{-1}$, is calculated as the inverse of $V$. We know that the inverse of
a diagonal matrix is the same matrix but with the diagonal elements reciprocated. Thus we have

\[V^{-1} \,=\,
\begin{bmatrix}1\\
&1\\
& &\ddots\\
& & &1/16\\
& & & &\ddots\\
& & & & &1\\
& & & & & &1
\end{bmatrix}\]

The weights we will supply to `lm()` will be a vector of 15 ones, followed by the value $1/16$,
followed by 7 ones. Instead of typing the number `1` 15 times, we can use the `rep()` function
which repeats a value $n$ times.

```{r}
w1 <- c(rep(1, 15), 1/16, rep(1, 7))
w1

# Check that it has the same length as the data (n=23)
length(w1)

# Check that the value at position 16 is correct
w1[16]
```

Now we can proceed with building the model.

```{r}
wm1 <- lm(Y ~ X, data=table2.1, weights=w1)
summary(wm1)
```

The equation of the fitted line is:

\[\widehat{Y}_{i} \,=\, 1.4421 \,+\, 0.3077X_{i}\]


### Problem (ii)

Suppose we are now told that the 16th observation has variance $25\sigma^{2}$ rather than
$\sigma^{2}$. Obtain the parameter estimates using weighted least squares.


### Solution

We repeat the process from before. Since the variance for the 16th observation has been scaled by
a factor of 25, the weight will be $1/25$.

```{r}
w2 <- c(rep(1, 15), 1/25, rep(1, 7))
w2

# Check that it has the same length as the data (n=23)
length(w2)

# Check that the value at position 16 is correct
w2[16]
```

Building a second model,

```{r}
wm2 <- lm(Y ~ X, data=table2.1, weights=w2)
summary(wm2)
```

The equation of the fitted line is:

\[\widehat{Y}_{i} \,=\, 1.4425 \,+\, 0.3075X_{i}\]

We can see that in changing our weight for the 16th observation from $1/16$ to $1/25$:

- The value of $\widehat{\beta}_{0}$ has increased (not by much)
- The value of $\widehat{\beta}_{1}$ has decreased (not by much)

The differences are more apparent when adjusting weights to the 23rd observation of this data set.
Hint: make a scatterplot!

First, let's add a column of indices to the data.

```{r}
table2.1 <- table2.1 %>%
  mutate(obs = 1:n(), .before = everything())

table2.1
```

Then we can make a scatterplot and label each point with its observation number.

```{r}
ggplot(table2.1, aes(x=X, y=Y, label=obs))+
  geom_point(alpha=0.4)+
  geom_text(hjust="outward", vjust="outward")
```

Observation 23 definitely stands out from the rest of the data!


## All possible models

To be consistent with the textbook, let us define $p$ as the total number of parameters in a model,
including the intercept. Suppose we are interested in plotting Mallows' $C_{p}$ against $p$ for
all possible models. 

*(Alternatively, we could consider plotting $C_{p}$ against $p+1$ where $p$ is the number of
non-intercept parameters).*

Returning to the `rock` data set, an outline of our plan will be as follows:

1. To compute all the possible models in a single line, we employ `leaps::regsubsets()`. In the
formula argument (`x`), we specify the formula `area ~ .`. This means fix `area` as the response
variable, and allow *any* other variables appearing in the data set to be added to the candidate
model (this is so that we don't need to manually type the names of the variables in our data set).
We also set `nbest=3` to force computation of **all** models when $p < 4$.

2. We then pass the result through a custom function that is derived from `broom::tidy.regsubsets()`
to obtain the required diagnostic information (including Mallows' $C_{p}$) as a tidy tibble. We need
a custom function since we also want the $SSE_{p}$ of each model in order to calculate the $MSE_{p}$
(or equivalently, the $S^{2}_{p}$).

3. We can then create the column, $p$, that counts the number of variables included in the model
(including the intercept). This involves some code that is a bit more complicated, but it is
essentially summing the number of `TRUE`s appearing in each row (i.e. we are performing a
horizontal sum rather than the usual vertical sum).

4. We then compute the mean square error, $S^{2}_{p}$, for each model.

5. We then drop the columns that we won't be using.

6. Finally, we can plot our results.


### A custom tidy function

We can access the source code used to tidy objects of class `regsubsets` by calling 
`broom:::tidy.regsubsets` (three colons).

```{r}
broom:::tidy.regsubsets
```

Consulting the documentation of `?leaps::regsubsets()`, we can create a custom function that also
includes the SSE of each model.

```{r}
tidy.regsubsets2 <- function(x, ...) {
  s <- summary(x)
  inclusions <- as_tibble(s$which)
  metrics <- with(s, tibble(r.squared = rsq, adj.r.squared = adjr2, 
                            BIC = bic, mallows_cp = cp, SSE_p = rss))
  bind_cols(inclusions, metrics)
}
```


### Getting the results

Recall that

\[\widehat{\sigma}^{2}_{p} \,=\, S^{2}_{p} \,=\, \frac{SSE_{p}}{n-p}\]

where $n$ is the number of observations in your data, and $p$ is the number of parameters including
the intercept.

```{r}
n <- nrow(rock)

all_models <- regsubsets(area ~ ., data=rock, method="exhaustive", nbest=3) %>%
  tidy.regsubsets2() %>%
  rowwise() %>%
  mutate(p = sum(c_across(where(is.logical)))) %>%
  ungroup() %>%
  mutate(s.squared_p = SSE_p / (n - p)) %>%
  select(-c(r.squared, BIC, SSE_p))

all_models
```

In choosing a model, we want a model that has a high adjusted $R^{2}$, $C_{p} \approx p$, low
$S^{2}_{p}$, and as few predictors as possible. From the results above, models 4 and 7 are quite
similar with model 7 (the full model) being only slightly better.


### Obtaining the values for the intercept-only model

We first fit the intercept-only model.

```{r}
rock_intercept_lm <- lm(area ~ 1, data=rock)
```

The formula for Mallows' $C_{p}$ is given on page 332 of the textbook:

\[C_{p} \,=\, \frac{SSE_{p}}{S^{2}} \,-\, (n - 2p)\]

where $S^{2}$ is the estimate of the error term variance for the maximal model $(p=4)$. Note that
the value for $n$ has already been initialised as a variable in our workspace. We can extract the
value of $S^{2}_{4}$ from our previous results using:

```{r}
s.squared_4 <- all_models %>%
  slice_tail(n=1) %>%
  pull(s.squared_p)
```

We have the relationship that

\[S^{2}_{1} \,=\, MSE_{1} \,=\, \frac{SSE_{1}}{\text{df}_{1}}\]

Rearranging gives

\[SSE_{1} \,=\, \text{df}_{1} * S^{2}_{1}\]

Mimicking the structure of `all_models`, the corresponding values for our intercept-only model are:

```{r}
intercept_lm_results <- tibble(
  `(Intercept)` = TRUE,
  peri = FALSE,
  shape = FALSE,
  perm = FALSE,
  adj.r.squared = summary(rock_intercept_lm)$adj.r.squared,
  s.squared_p = summary(rock_intercept_lm)$sigma^2,
  p = summary(rock_intercept_lm)$df[1],
  df = summary(rock_intercept_lm)$df[2],
  mallows_cp = (s.squared_p * df) / s.squared_4 - (n - 2 * p)
) %>%
  select(where(is.logical), adj.r.squared, mallows_cp, p, s.squared_p)

intercept_lm_results
```

Binding this to the main output:

```{r}
all_models <- bind_rows(intercept_lm_results, all_models)

all_models
```


### Plotting the results

```{r}
ggplot(all_models, aes(x=p, y=mallows_cp))+
  geom_point()+
  labs(caption="p is the number of parameter estimates including the intercept")
```


## Stepwise selection

Let's revisit the penguins data from 
[palmerpenguins](https://allisonhorst.github.io/palmerpenguins/), specifically the subset of
penguins of species Adelie from the island Biscoe.

```{r}
adeliebiscoe <- palmerpenguins::penguins %>%
  drop_na() %>%
  filter(species == "Adelie", island == "Biscoe") %>%
  rename(
    bill_length = bill_length_mm,
    bill_depth = bill_depth_mm,
    flipper_length = flipper_length_mm,
    body_mass = body_mass_g,
  ) %>%
  select(body_mass, bill_length, bill_depth, flipper_length)
```

For the following, we will use:

\[\alpha_{\text{entry}} = 0.05, \quad \alpha_{\text{exit}} = 0.05\]


### Building the base model

We are interested in performing stepwise selection using `body_mass` as the response. As recommended
in the lecture notes, we commence the model building process by including the predictor that is most
correlated to `body_mass`.

```{r}
cor(adeliebiscoe)
```

`bill_depth` is the most correlated with `body_mass` (remember to look at the absolute value of the
correlation).

```{r}
step1 <- lm(body_mass ~ bill_depth, data=adeliebiscoe)

summary(step1)
```

From the summary output, the $t$-test of `bill_depth` is significant so we can continue considering
predictors to add to the model.


### Adding a predictor, round 1

Note that we can perform the partial $F$-tests of single variable additions using the usual method
of building the candidate model and then calling `anova()` on the reduced and full models.

```{r}
step2a <- lm(body_mass ~ bill_depth + bill_length, data=adeliebiscoe)
step2b <- lm(body_mass ~ bill_depth + flipper_length, data=adeliebiscoe)

anova(step1, step2a)
anova(step1, step2b)
```

This can become a lot of work at each step and can become tedious if there are many predictors to
consider at each step. Instead, we can perform the partial $F$-tests of single variable additions
using the `add1()` function.

```{r}
add1(step1, ~ . + bill_length + flipper_length, test="F")
```

The first argument to `add1()` is the base model which we are considering adding predictors to. The
second argument is the scope, where we list all predictors (using formula syntax) that are eligible
to be added to the main model. Notice that the results of the partial $F$-tests are identical to
the previous results when we had constructed the models individually (`step2a` and `step2b`).

The predictor that gets added to our model is the one associated to the highest $F$-value. Since
each of these $F$-values have the same numerator and denominator degrees of freedom, this is also
equivalent to adding the predictor that has the smallest $p$-value associated to its partial
$F$-test (so long as it is less than $\alpha_{\text{entry}}$). Since the $F$-test of adding
`bill_length` to the base model results in the smallest $p$-value that is less than
$\alpha_{\text{entry}}$, `bill_length` is added to the base model.

```{r}
step2 <- update(step1, . ~ . + bill_length)
```


### Dropping a predictor, round 1

Now we check whether there are any predictors that can be dropped. As before, we can construct
multiple models that differ by single term deletions and perform the partial $F$-tests by
calling `anova()` on the reduced and full models. We can also use the equivalent of `add1()` for
single term deletions, which is `drop1()`.

```{r}
drop1(step2, ~ ., test="F")
```

The first argument to `drop1()` is the model from which we are considering dropping variables from.
The second argument is the scope (similar to `add1()`), where we list all predictors (using formula
syntax) that are eligible to be dropped from the main model. Here, all predictors currently in the
model under consideration are eligible to be dropped.

Recall that the hypotheses associated to dropping a single predictor are:

\[H_{0}: \beta_{1} = 0, \quad H_{A}: \beta_{1} \neq 0\]
\[H_{0}: \beta_{2} = 0, \quad H_{A}: \beta_{2} \neq 0\]

For a predictor to be dropped from the model, we need to fail to reject the null hypothesis. So we
are looking for small $F$-values, or equivalently, large $p$-values that are greater than
$\alpha_{\text{exit}}$. Since we do not have any here, we do not drop any of the predictors from the
current model.


### Adding a predictor, round 2

At this stage, we only have one more variable left in our data set that can be added to the model.

```{r}
add1(step2, ~ . + flipper_length, test="F")
```

Since the $p$-value for this $F$-test is less than $\alpha_{\text{entry}}$, we add `flipper_length`
into the main model.

```{r}
step3 <- update(step2, . ~ . + flipper_length)
```


### Dropping a predictor, round 2

```{r}
drop1(step3, ~ ., test="F")
```

As before, predictors that should be considered for dropping are those that have a $p$-value greater
than $\alpha_{\text{exit}}$. Since we do not have any, we do not drop any of the predictors in our
model.


### The final model

```{r}
summary(step3)
```

The final model is:

\[\widehat{Y}_{i} \,=\, -6122.000 \,+\, 165.196X_{1,\,i} + 70.743X_{2,\,i} \,+\, 21.397X_{3,\,i}\]

Note that our adjusted R-squared value is 0.6246, which is not terrible, but not great. However,
recall that in the last lab, removing the intercept from the model did result in a jump in the
adjusted R-squared from 0.6246 to 0.9905.


## Backward selection

In backward selection, we start with the full model and at each step, consider dropping predictors.
As we had seen for the Adelie Biscoe data, performing the $F$-test for single term deletions on the
model that contained all predictors, none were eligible for dropping. Therefore, for this example,
let us consider another data set &mdash; the
[Scooby Doo data set](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-07-13/readme.md),
which is abundant in variables.

```{r, eval=FALSE}
scoobydoo <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-13/scoobydoo.csv")
```

```{r, echo=FALSE, eval=FALSE}
download.file(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-13/scoobydoo.csv",
  "./data/scoobydoo.csv"
)
```

```{r, echo=FALSE}
scoobydoo <- read_csv("./data/scoobydoo.csv")
```

```{r}
names(scoobydoo)
```


### Data prep

For this example, we will only keep a subset of the variables for the sake of brevity. In addition,
some of the variables that should be of type numeric were read in as strings due to the presence
of the string `"NULL"` (note that the string `"NULL"` is not the same as `NA`). For example:

```{r}
scoobydoo %>%
  count(jinkies)
```

Therefore, we want to remove all episodes that contain `"NULL"` values in selected columns and
coerce the remaining values to numeric.

```{r}
scoobydoo <- scoobydoo %>%
  select(
    imdb, engagement, run_time, monster_amount, suspects_amount, culprit_amount,
    jeepers, jinkies, my_glasses, zoinks
  ) %>%
  filter(across(where(is.character), ~ . != "NULL")) %>%
  mutate(across(where(is.character), as.numeric))
```

In the line containing `filter()`, we are filtering over all character-type columns to only retain
episodes that do not contain any `"NULL"` values. Specifying the filtering condition this way
allows us to do less typing. Otherwise, we would need to type out all the variables of interest:

```{r, eval=FALSE}
filter(imdb != "NULL", jeepers != "NULL", jinkies != "NULL", etc.)
```

Note that the portion `~ . != "NULL"` is a lambda function / anonymous function, not a mathematical
formula, and is the short-hand equivalent to:

```{r, eval=FALSE}
function(x) {
  x != "NULL"
}
```

Previewing our data:

```{r, R.options=list(width=500)}
scoobydoo
```

Now we are ready to start building models. We are interested in creating a model that predicts
the IMDB rating. As we are performing backward selection, we start with the model containing all
predictors. For the following example, we will use $\alpha_{\text{exit}} = 0.05$.

```{r}
back1 <- lm(imdb ~ ., data=scoobydoo)

summary(back1)
```


### Dropping a predictor, round 1

```{r}
drop1(back1, ~ ., test="F")
```

Remember that when we are dropping predictors, in the related hypothesis test, we want to fail to
reject the null hypothesis. Therefore we are looking for $p$-values that are greater than
$\alpha_{\text{exit}} = 0.05$. From the output above, there are many predictors that are eligible to
be dropped. Therefore, we select the predictor corresponding to the largest $p$-value greater than
$\alpha_{\text{exit}} = 0.05$. This is `culprit_amount` with a $p$-value of 0.643.

```{r}
back2 <- update(back1, . ~ . - culprit_amount)
```


### Dropping a predictor, round 2

```{r}
drop1(back2, ~ ., test="F")
```

The variable with the largest $p$-value greater than $\alpha_{\text{exit}} = 0.05$ is `zoinks`.

```{r}
back3 <- update(back2, . ~ . - zoinks)
```


### Dropping a predictor, round 3

```{r}
drop1(back3, ~ ., test="F")
```

The variable with the largest $p$-value greater than $\alpha_{\text{exit}} = 0.05$ is `jeepers`.

```{r}
back4 <- update(back3, . ~ . - jeepers)
```


### Dropping a predictor, round 4

```{r}
drop1(back4, ~ ., test="F")
```

The variable with the largest $p$-value greater than $\alpha_{\text{exit}} = 0.05$ is `engagement`.

```{r}
back5 <- update(back4, . ~ . - engagement)
```


### Dropping a predictor, round 5

```{r}
drop1(back5, ~ ., test="F")
```

The variable with the largest $p$-value greater than $\alpha_{\text{exit}} = 0.05$ is `my_glasses`.

```{r}
back6 <- update(back5, . ~ . - my_glasses)
```


### Dropping a predictor, round 6

```{r}
drop1(back6, ~ ., test="F")
```

The remaining single term deletions have $p$-values that are less than
$\alpha_{\text{exit}} = 0.05$. Therefore, we stop here.


### The final model

```{r}
summary(back6)
```

The final model is:

\[\widehat{Y}_{i} \,=\, 8.2645 \,-\, 0.0114X_{1,\,i} \,-\, 0.0524X_{2,\,i} - 0.0647X_{3,\,i} - 0.0579X_{4,\,i}\]

Interestingly, this says that the number of jinkies in an episode has a (small) negative impact on
the IMDB rating &#x1F602;!

While all the remaining predictors in our model are significant, we have a very poor adjusted
R-squared value of 0.3336. This means that we have only explained 33.36% of the variation in
IMDB ratings by using these predictors. In fact, our original model with all the predictors had an
adjusted R-squared value of 0.3340. As such, the final model still has room for improvement
(though do keep in mind that only a small subset of variables were included in this example for the
sake of brevity).

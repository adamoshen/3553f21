[{"path":"index.html","id":"session-info","chapter":"Session info","heading":"Session info","text":"","code":"R version 4.0.3 (2020-10-10)\nRStudio version 1.4.1106\nWindows 10 x64##  package        * version date       lib source        \n##  bookdown         0.24    2021-09-02 [1] CRAN (R 4.0.5)\n##  broom            0.7.6   2021-04-05 [1] CRAN (R 4.0.5)\n##  bslib            0.2.5.1 2021-05-18 [1] CRAN (R 4.0.3)\n##  downlit          0.2.1   2020-11-04 [1] CRAN (R 4.0.5)\n##  dplyr            1.0.7   2021-06-18 [1] CRAN (R 4.0.5)\n##  ellipse          0.4.2   2020-05-27 [1] CRAN (R 4.0.5)\n##  ggplot2          3.3.5   2021-06-25 [1] CRAN (R 4.0.5)\n##  knitr            1.31    2021-01-27 [1] CRAN (R 4.0.4)\n##  leaps            3.1     2020-01-16 [1] CRAN (R 4.0.5)\n##  palmerpenguins   0.1.0   2020-07-23 [1] CRAN (R 4.0.5)\n##  readr            2.1.0   2021-11-11 [1] CRAN (R 4.0.3)\n##  rmarkdown        2.9     2021-06-15 [1] CRAN (R 4.0.5)\n##  svglite          2.0.0   2021-02-20 [1] CRAN (R 4.0.5)\n##  tibble           3.1.6   2021-11-07 [1] CRAN (R 4.0.3)\n##  tidyr            1.1.4   2021-09-27 [1] CRAN (R 4.0.5)\n## \n## [1] C:/Users/Adam/Documents/R/win-library/4.0\n## [2] C:/Program Files/R/R-4.0.3/library"},{"path":"lab-1-september-15.html","id":"lab-1-september-15","chapter":"1 Lab 1 â€” September 15","heading":"1 Lab 1 â€” September 15","text":"","code":""},{"path":"lab-1-september-15.html","id":"review-of-r-basics","chapter":"1 Lab 1 â€” September 15","heading":"1.1 Review of R basics","text":"R name programming language. RStudio name integrated development\nenvironment (IDE). application running RStudio.already , R can downloaded .\nRStudio (desktop version) can downloaded\n. R installed \ninstalling RStudio.already R installed device, ensure version >= 4.0.0. Otherwise,\nuse links install latest version.","code":""},{"path":"lab-1-september-15.html","id":"configure-rstudio-settings","chapter":"1 Lab 1 â€” September 15","heading":"1.1.1 Configure RStudio settings","text":"settings can found Tools -> Global Options....Uncheck boxes modify dropdowns mention saving items upon exit restoring items\nupon startup. ensure RStudio starts fresh session time.","code":""},{"path":"lab-1-september-15.html","id":"console-pane","chapter":"1 Lab 1 â€” September 15","heading":"1.1.2 Console pane","text":"RStudio application opened, Console pane occupy entirety left\nside. (Open image new tab small)code typed Console evaluated immediately upon pressing Enter.\nneed evaluate multi-line code, Shift + Enter used \ncreate line breaks.Console often used evaluate code may part main analyses. \nincludes checking work, testing code, modifying settings, etc.","code":""},{"path":"lab-1-september-15.html","id":"source-pane","chapter":"1 Lab 1 â€” September 15","heading":"1.1.3 Source pane","text":"RStudio application opened, Source pane hidden default. Source\npane modify R scripts type main code analyses. strongly\nrecommended save important code (code may need revisit later\ndate) inside R script file. new R script can created using:File -> New File -> R Script, orCtrl + Shift + N, orClicking  selecting \"R Script\"Source pane appear top left open R script.\n(Open image new tab small)Code typed script file evaluated upon pressing Enter. Code must \nsent Source Console evaluation.run single line code, place typing cursor anywhere line press\nCtrl + Enter.run multiple lines code, highlight desired lines press Ctrl +\nEnter.run entire script start finish, click  .","code":""},{"path":"lab-1-september-15.html","id":"environment-pane","chapter":"1 Lab 1 â€” September 15","heading":"1.1.4 Environment pane","text":"Environment pane top right pane RStudio. pane shows variables \ninitialised current R session. variables current R session can removed\nclicking broom icon. Alternatively, switching Grid view List view allow \nremove selected variables.","code":""},{"path":"lab-1-september-15.html","id":"comments","chapter":"1 Lab 1 â€” September 15","heading":"1.1.5 Comments","text":"Comments can added R script prepending # symbol.","code":""},{"path":"lab-1-september-15.html","id":"variables","chapter":"1 Lab 1 â€” September 15","heading":"1.1.6 Variables","text":"Variables created using convention name <- value.variable successfully initialised, appear Environment pane. \nsimple variables, value variable can seen Environment pane. variables\nwhose values visible Environment pane, values can checked calling\nConsole:using RStudio's built-viewer typing Console:naming variables, try use names already exist R data sets functions. \ncan checked partially typing variable name seeing suggestions come . \nexample, reading data, name variable data since already exists\ndata() function R.","code":"\nvar1 <- 3\nvar1## [1] 3\nView(var1)"},{"path":"lab-1-september-15.html","id":"vectors","chapter":"1 Lab 1 â€” September 15","heading":"1.1.7 Vectors","text":"Vectors univariate data structures. Vectors can created using c() function \nelements separated commas.Note examplevar1 actually vector length 1.calling vectors Console, note numbers square brackets \ncorresponding output represent position number first element line.","code":"\nnums <- c(9, 12, 20)\nwords <- c(\"apple\", \"orange\", \"banana\")\nvar1 <- 3\nvar2 <- 1:30\n\nvar2##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n## [26] 26 27 28 29 30"},{"path":"lab-1-september-15.html","id":"extracting-elements","chapter":"1 Lab 1 â€” September 15","heading":"1.1.7.1 Extracting elements","text":"elements vectors can extracted supplying vector positive integers inside square\nbrackets. Note R, indices begin 1.","code":"\nvar2[7]## [1] 7\nvar2[c(2, 7, 9, 11, 20)]## [1]  2  7  9 11 20"},{"path":"lab-1-september-15.html","id":"deleting-elements","chapter":"1 Lab 1 â€” September 15","heading":"1.1.7.2 Deleting elements","text":"elements vectors can deleted supplying vector negative integers placing \nnegative outside vector positive integers, inside square brackets.\nNote R, indices begin 1.","code":"\nvar2[-7]##  [1]  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n## [26] 27 28 29 30\nvar2[c(-2, -7, -9, -11, -20)]##  [1]  1  3  4  5  6  8 10 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27 28 29 30\nvar2[-c(2, 7, 9, 11, 20)]##  [1]  1  3  4  5  6  8 10 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27 28 29 30"},{"path":"lab-1-september-15.html","id":"data-frames","chapter":"1 Lab 1 â€” September 15","heading":"1.1.8 Data frames","text":"Data frames multivariate data structures. Data frames can thought vertical\nconcatentation vectors. cases, column data frame contains values \nsingle variable row data frame contains values variable single\nobservation. example, consider following data set:example , column contains values single variable row contains\nvalues variable single observation (patient).Things note:Data frames can created supplying name-value pairs data.frame() function.Name-value pairs supplied using = rather <-.name-value pair must length. Missing values can represented using NA.","code":"\npatients <- data.frame(\n  patient_id = c(\"123abc\", \"28fnr8\", \"02jr8d\", \"r82j45\", \"t90ro5\"),\n  height = c(181, 145, 190, 210, 94),\n  mass = c(85, 72, 82, 90, 60),\n  sex = c(\"M\", \"F\", \"M\", \"F\", \"F\")\n)\n\npatients##   patient_id height mass sex\n## 1     123abc    181   85   M\n## 2     28fnr8    145   72   F\n## 3     02jr8d    190   82   M\n## 4     r82j45    210   90   F\n## 5     t90ro5     94   60   F"},{"path":"lab-1-september-15.html","id":"extracting-columns","chapter":"1 Lab 1 â€” September 15","heading":"1.1.8.1 Extracting columns","text":"Columns data frame can extracted vectors using dollar sign ($).","code":"\npatients$height## [1] 181 145 190 210  94\npatients$sex## [1] \"M\" \"F\" \"M\" \"F\" \"F\""},{"path":"lab-1-september-15.html","id":"extracting-rows-and-subsets","chapter":"1 Lab 1 â€” September 15","heading":"1.1.8.2 Extracting rows and subsets","text":"Thinking data frames matrices, can subsetted supplying row column\npositions/names separated comma within square brackets:","code":"\n# Extract row 3 and all columns\npatients[3, ]##   patient_id height mass sex\n## 3     02jr8d    190   82   M\n# Extract rows 1, 4, and 5, and columns 1 and 3\npatients[c(1, 4, 5), c(1, 3)]##   patient_id mass\n## 1     123abc   85\n## 4     r82j45   90\n## 5     t90ro5   60\n# Extract rows 1, 4, and 5, and columns \"patient_id\" and \"mass\"\npatients[c(1, 4, 5), c(\"patient_id\", \"mass\")]##   patient_id mass\n## 1     123abc   85\n## 4     r82j45   90\n## 5     t90ro5   60\n# Extract everything *except* rows 2 and 3, and columns 2 and 3\npatients[-c(2, 3), -c(2, 3)]##   patient_id sex\n## 1     123abc   M\n## 4     r82j45   F\n## 5     t90ro5   F"},{"path":"lab-1-september-15.html","id":"the-working-directory","chapter":"1 Lab 1 â€” September 15","heading":"1.1.9 The working directory","text":"working directory folder path working . path current\nworking directory can found immediately underneath Console tab heading.opening first instance RStudio using application directly, working directory \nautomatically set \n\"Default working directory (project)\". first\ninstance RStudio opened opening R script file (files ending .R), working directory\nautomatically set folder R script file located.Note opening first instance RStudio, scripts opened \nchange working directory.working directory important keep track reading writing files. sake \norganization, may case following folder structure:dot represents currently (working directory).example , R scripts contained working directory, data output\nfiles folders working directory. , reading files need specify\nread \"data\" folder. Similarly, writing files, need specify\nwritten \"output\" folder.able , need talk file folder paths specify .","code":".\nâ”œâ”€â”€ data\nâ”‚   â””â”€â”€ my_data.txt\nâ”œâ”€â”€ output\nâ”‚   â”œâ”€â”€ output1.txt\nâ”‚   â””â”€â”€ model.RDS\nâ”œâ”€â”€ script1.R\nâ””â”€â”€ script2.R"},{"path":"lab-1-september-15.html","id":"paths","chapter":"1 Lab 1 â€” September 15","heading":"1.1.10 Paths","text":"path string (.e. needs surrounded quotes) describes location file \nfolder device. Paths become extremely important reading writing files R. Paths\ncan broken two categories: absolute relative.","code":""},{"path":"lab-1-september-15.html","id":"absolute-paths","chapter":"1 Lab 1 â€” September 15","heading":"1.1.10.1 Absolute paths","text":"absolute path gives full location file/folder device, irrespective \ncurrent working directory, e.g.use absolute paths creates problems.Suppose Windows include path like \"C:/Users/Adam/Desktop/blah.R\" \nscript. execute code given Windows machine, likely\nwork since name probably Adam blah.R may located Desktop\nlike mine.Suppose Windows include path like \"C:/Users/Adam/Desktop/blah.R\" \nscript. execute code given Windows machine, likely\nwork since name probably Adam blah.R may located Desktop\nlike mine.users Windows, addition folders possibly existing, likely\n\"C drive\".users Windows, addition folders possibly existing, likely\n\"C drive\"., use absolute paths recommended complicates sharing code\ndevices platform-dependent.","code":"\"C:/Users/Adam/Desktop/blah.R\"\n\"/Users/John/Documents/lab1.txt\""},{"path":"lab-1-september-15.html","id":"relative-paths","chapter":"1 Lab 1 â€” September 15","heading":"1.1.10.2 Relative paths","text":"relative path gives partial location file/folder relative working directory.\nAlthough use relative paths still assumes identical partial folder structure, \nassume identical folder structure root path.two helpers specifying relative paths: dot, \".\", double-dot, \"..\". \ndot, \".\", can thought shorthand \"\", current working directory. \ndouble-dot, \"..\", means \"go one level\".Example 1:Suppose working directory \"C:/Users/Adam/Desktop\" structure:Inside script1.R, want read data found my_data.txt. can done using:rather using:Example 2:Suppose working directory \"C:/Users/Adam/Desktop/model\" surrounding folder\nstructure:Inside model.R, want read data found my_data.txt. can done using:rather using:","code":"C:/Users/Adam/Desktop **you are here**\nâ”‚\nâ”œâ”€â”€ my_data.txt\nâ””â”€â”€ script1.R\nmy_data <- read.table(\"my_data.txt\") \n## OR ##\nmy_data <- read.table(\"./my_data.txt\")\nmy_data <- read.table(\"C:/Users/Adam/Desktop/my_data.txt\")C:/Users/Adam/Desktop\nâ”‚\nâ”œâ”€â”€ data\nâ”‚   â””â”€â”€ my_data.txt\nâ”œâ”€â”€ model **you are here**\nâ”‚   â”œâ”€â”€ output1.txt\nâ”‚   â””â”€â”€ model.R\nâ”œâ”€â”€ output\nâ”‚   â”œâ”€â”€ output1.txt\nâ”‚   â””â”€â”€ output2.RDS\nâ””â”€â”€ other.R\nmy_data <- read.table(\"../data/my_data.txt\")\nmy_data <- read.table(\"C:/Users/Adam/Desktop/data/my_data.txt\")"},{"path":"lab-1-september-15.html","id":"reading-in-data","chapter":"1 Lab 1 â€” September 15","heading":"1.1.11 Reading in data","text":"course, () data read .txt file header. \ncode use :instances, may need:","code":"\nmy_data <- read.table(\"path/to/data/file.txt\", header=TRUE)\nmy_data <- read.table(\"path/to/data/file.txt\", header=TRUE, stringsAsFactors=TRUE)"},{"path":"lab-1-september-15.html","id":"accessing-the-built-in-documentation","chapter":"1 Lab 1 â€” September 15","heading":"1.1.12 Accessing the built-in documentation","text":"Prepending function built-data set question mark bring associated\ndocumentation. example, Console, try:","code":"\n?read.table\n?lm\n?rock"},{"path":"lab-1-september-15.html","id":"introduction-to-simple-linear-regression","chapter":"1 Lab 1 â€” September 15","heading":"1.2 Introduction to simple linear regression","text":"Note: content contained following sections aid answering Assignment 1\nQuestion 5.","code":""},{"path":"lab-1-september-15.html","id":"loading-the-data","chapter":"1 Lab 1 â€” September 15","heading":"1.2.1 Loading the data","text":"data set using can found file named table1.1_DS.txt. Since \ntext file header, use read.table() read specify header=TRUE.data contained folder called \"data\" current working directory. path use\nread data depend saved data.can get dimensions data set using functions dim(), nrow(), ncol().can get column names data set using names() function.can preview data read using head() function. head function useful\nespecially larger data sets printing entire data set may desirable. \ndefault, function return first six rows data.can also use RStudio's built-object viewer view full data set. Environment\npane:List view, click row corresponding variable containing data.Grid view, click row corresponding variable containing data \n\"Value\" heading.Alternatively, can use View() function:","code":"\ntable1.1 <- read.table(\"./data/table1.1_DS.txt\", header=TRUE)\ndim(table1.1)## [1] 25  2\nnrow(table1.1)## [1] 25\nncol(table1.1)## [1] 2\nnames(table1.1)## [1] \"y\" \"x\"\nhead(table1.1)##       y    x\n## 1 10.98 35.3\n## 2 11.13 29.7\n## 3 12.51 30.8\n## 4  8.40 58.8\n## 5  9.27 61.4\n## 6  8.73 71.3\nView(table1.1)"},{"path":"lab-1-september-15.html","id":"creating-the-data-manually","chapter":"1 Lab 1 â€” September 15","heading":"1.2.2 Creating the data manually","text":"Assignment 1 Question 5, given data ordered pairs need manually\nenter R. Using first ten observations table1.1 example, can create \ndata frame manually:(continue using full Table 1.1 demonstrations ).","code":"\ntable1.1_manual <- data.frame(\n  y = c(10.98, 11.13, 12.51, 8.40, 9.27, 8.73, 6.36, 8.50, 7.82, 9.14),\n  x = c(35.3, 29.7, 30.8, 58.8, 61.4, 71.3, 74.4, 76.7, 70.7, 57.5)\n)\n\nhead(table1.1_manual)##       y    x\n## 1 10.98 35.3\n## 2 11.13 29.7\n## 3 12.51 30.8\n## 4  8.40 58.8\n## 5  9.27 61.4\n## 6  8.73 71.3"},{"path":"lab-1-september-15.html","id":"fitting-the-slr-model","chapter":"1 Lab 1 â€” September 15","heading":"1.2.3 Fitting the SLR model","text":"Linear regression models fit using lm() function. first need fit model\\[Y_{} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1}X_{} \\,+\\, \\varepsilon_{}, \\quad =1,\\ldots,n\\]order determine whether\\[Y_{} \\,=\\, \\beta_{0} \\,+\\, \\varepsilon_{}, \\quad =1,\\ldots,n\\]appropriate model.code says build linear regression model using y dependent (response)\nvariable, x independent (predictor) variable, variables y x \nfound within table1.1 data variable. Although need specify intercept \nformula specification, linear regression models built lm() include intercept default.","code":"\nslr_model <- lm(y ~ x, data=table1.1)"},{"path":"lab-1-september-15.html","id":"inspecting-the-model","chapter":"1 Lab 1 â€” September 15","heading":"1.2.4 Inspecting the model","text":"can obtain summary linear regression model using summary() function.information , fitted regression line equation:\\[\\widehat{Y}_{} \\,=\\, 13.62 \\,-\\, 0.08X_{} \\quad =1,\\ldots,n\\]","code":"\nsummary(slr_model)## \n## Call:\n## lm(formula = y ~ x, data = table1.1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.6789 -0.5291 -0.1221  0.7988  1.3457 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 13.62299    0.58146  23.429  < 2e-16 ***\n## x           -0.07983    0.01052  -7.586 1.05e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8901 on 23 degrees of freedom\n## Multiple R-squared:  0.7144, Adjusted R-squared:  0.702 \n## F-statistic: 57.54 on 1 and 23 DF,  p-value: 1.055e-07"},{"path":"lab-1-september-15.html","id":"is-an-intercept-only-model-appropriate","chapter":"1 Lab 1 â€” September 15","heading":"1.2.5 Is an intercept-only model appropriate?","text":"determine whether intercept-model appropriate, need test hypotheses:\\[H_{0}: \\beta_{1} \\,=\\, 0, \\quad H_{}: \\beta_{1} \\,\\neq\\, 0\\]value test statistic can read summary table value -7.586 \ncorresponding \\(p\\)-value 1.05e-07. Using significance level \\(\\alpha = 0.05\\), since\n\\(p\\)-value less \\(\\alpha\\), reject null hypothesis conclude \nevidence support claim \\(\\beta_{1}\\) different zero., consider intercept-model.\\(p\\)-value greater \\(\\alpha = 0.05\\) failed reject null\nhypothesis? fit intercept-model?Aside: note (Intercept) row, t-value \\(p\\)-value correspond testing \nhypotheses:\\[H_{0}: \\beta_{0} \\,=\\, 0, \\quad H_{}: \\beta_{0} \\,\\neq\\, 0\\]","code":""},{"path":"lab-1-september-15.html","id":"the-rock-data-set","chapter":"1 Lab 1 â€” September 15","heading":"1.2.6 The rock data set","text":"fit intercept-model, let us consider rock data set built R. data\nset can loaded using:can preview data usual:","code":"\ndata(rock)\nhead(rock)##   area    peri     shape perm\n## 1 4990 2791.90 0.0903296  6.3\n## 2 7002 3892.60 0.1486220  6.3\n## 3 7558 3930.66 0.1833120  6.3\n## 4 7352 3869.32 0.1170630  6.3\n## 5 7943 3948.54 0.1224170 17.1\n## 6 7979 4010.15 0.1670450 17.1"},{"path":"lab-1-september-15.html","id":"fitting-a-slr","chapter":"1 Lab 1 â€” September 15","heading":"1.2.7 Fitting a SLR","text":"create simple linear regression model using area dependent (response) variable \nshape independent (predictor) variable.equation fitted line :\\[\\widehat{Y}_{} \\,=\\, 8465 \\,-\\, 5855X_{}, \\quad =1,\\ldots,n\\]Testing hypotheses:\\[H_{0}: \\beta_{1} \\,=\\, 0, \\quad H_{}: \\beta_{1} \\,\\neq\\, 0\\]corresponding \\(p\\)-value 0.215. Using significance level \\(\\alpha = 0.05\\), since \n\\(p\\)-value greater 0.05, fail reject null hypothesis. , insufficient\nevidence support claim \\(\\beta_{1}\\) non-zero.Now fit intercept-model!","code":"\nrock_slr_model <- lm(area ~ shape, data=rock)\n\nsummary(rock_slr_model)## \n## Call:\n## lm(formula = area ~ shape, data = rock)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6101.6 -1512.3   104.6  1765.3  5152.9 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     8465       1087   7.788 6.08e-10 ***\n## shape          -5855       4660  -1.256    0.215    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2667 on 46 degrees of freedom\n## Multiple R-squared:  0.03318,    Adjusted R-squared:  0.01216 \n## F-statistic: 1.579 on 1 and 46 DF,  p-value: 0.2153"},{"path":"lab-1-september-15.html","id":"fitting-an-intercept-only-model","chapter":"1 Lab 1 â€” September 15","heading":"1.2.8 Fitting an intercept-only model","text":"One way fitting intercept-model use lm() .dependent (response) variable remains area, right-hand side formula just\n1 indicate intercept.second way fitting intercept-model take advantage fact already\nbuilt simple linear regression model (rock_slr_model) obtain intercept-model \njust need \"remove\" shape variable formula specification. can accomplished\nusing update() function.formula specification , . refers \"everything already \". ,\n. left ~ refers area, . right ~ refers \n1 + shape (1 usually written since linear regression models fit \nintercept default, still exists!). Therefore, code says \nrock_intercept_model \"update\" rock_slr_model modification \nformula now area ~ 1 + shape - shape, resulting area ~ 1. can verify \nfact needed calling:update() function extremely useful start working multiple linear\nregression models variable selection procedures later course sometimes\neasier specify variables want rather ones want.output , equation fitted line :\\[\\widehat{Y}_{} \\,=\\, 7187.7, \\quad =1,\\ldots,n\\]","code":"\nrock_intercept_model <- lm(area ~ 1, data=rock)\nrock_intercept_model <- update(rock_slr_model, formula = . ~ . - shape)\nformula(rock_intercept_model)## area ~ 1\nsummary(rock_intercept_model)## \n## Call:\n## lm(formula = area ~ 1, data = rock)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6171.7 -1882.5   299.3  1681.8  5024.3 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   7187.7      387.4   18.55   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2684 on 47 degrees of freedom"},{"path":"lab-1-september-15.html","id":"visualising-the-intercept-only-fit","chapter":"1 Lab 1 â€” September 15","heading":"1.2.9 Visualising the intercept-only fit","text":"can draw scatterplot data area y-axis shape x-axis, \noverlay intercept-fit. drawing lines base-R graphics, recall need sort\nx-values (reorder corresponding y-values)! example, since fitted values\nconstant values shape, need reorder fitted values \ncorresponding shape values â€” shape values need sorted. However, \nsake consistency, sort shape values reorder corresponding fitted values.fitted values intercept-model can obtained wrapping model \nfitted() function.Now, making plot:","code":"\nfitted(rock_intercept_model)##        1        2        3        4        5        6        7        8 \n## 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 \n##        9       10       11       12       13       14       15       16 \n## 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 \n##       17       18       19       20       21       22       23       24 \n## 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 \n##       25       26       27       28       29       30       31       32 \n## 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 \n##       33       34       35       36       37       38       39       40 \n## 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 \n##       41       42       43       44       45       46       47       48 \n## 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729 7187.729\nsorted_shape <- sort(rock$shape)\nordered_fitted <- fitted(rock_intercept_model)[order(rock$shape)]\n\nplot(area ~ shape, data=rock, main=\"Intercept-only model\")\nlines(x=sorted_shape, y=ordered_fitted, col=\"darkred\", lwd=2)"},{"path":"lab-1-september-15.html","id":"visual-diagnostics","chapter":"1 Lab 1 â€” September 15","heading":"1.2.10 Visual diagnostics","text":"","code":""},{"path":"lab-1-september-15.html","id":"the-residual-vs.-predictor-plot","chapter":"1 Lab 1 â€” September 15","heading":"1.2.10.1 The residual vs. predictor plot","text":"identify potential issues intercept-model, can make plot residuals,\n\\(e_{} \\,=\\, Y_{} \\,-\\, \\widehat{Y}_{}\\), predictor values (shape).residual values intercept-model can obtained wrapping model \nresid() function.Now, making plot:assumptions linear regression, discernable patterns/trends \nplotted points. addition, plot look like random scatter \\(y=0\\) constant\nvariance. plot , appear patterns/trends \nplotted points. plot appears random scatter \\(y=0\\) constant variance.","code":"\nresid(rock_intercept_model)##          1          2          3          4          5          6          7 \n## -2197.7292  -185.7292   370.2708   164.2708   755.2708   791.2708  2145.2708 \n##          8          9         10         11         12         13         14 \n##  1021.2708  1205.2708  -762.7292  2176.2708  1436.2708  3463.2708  1680.2708 \n##         15         16         17         18         19         20         21 \n##  2229.2708  1686.2708  3774.2708  3555.2708  4690.2708  2679.2708   650.2708 \n##         22         23         24         25         26         27         28 \n##  4688.2708  5024.2708  1045.2708  -827.7292 -2994.7292   228.2708 -1941.7292 \n##         29         30         31         32         33         34         35 \n##  -678.7292 -2292.7292  -412.7292   706.2708 -1207.7292 -1869.7292   204.2708 \n##         36         37         38         39         40         41         42 \n##   706.2708 -3718.7292 -5719.7292 -3663.7292 -1920.7292 -2139.7292 -6171.7292 \n##         43         44         45         46         47         48 \n## -1582.7292  1605.2708 -3712.7292 -5536.7292 -1673.7292  2530.2708\nplot(\n  resid(rock_intercept_model) ~ shape, data=rock,\n  main=\"Residuals vs Predictor\", xlab=\"shape\", ylab=\"residuals\"\n)"},{"path":"lab-1-september-15.html","id":"the-residual-vs.-fitted-plot","chapter":"1 Lab 1 â€” September 15","heading":"1.2.10.2 The residual vs. fitted plot","text":"identify potential issues intercept-model, can also make plot residuals,\n\\(e_{} \\,=\\, Y_{} \\,-\\, \\widehat{Y}_{}\\), fitted values., discernable patterns/trends plotted points, plot\nlook like random scatter \\(y=0\\) constant variance. plot look bit\nstrange, fitted line constant! residuals appear scattered\n\\(y=0\\) equal variance, seem issues .","code":"\nplot(\n  resid(rock_intercept_model) ~ fitted(rock_intercept_model),\n  main=\"Residuals vs Fitted\", xlab=\"fitted values\", ylab=\"residuals\"\n)"},{"path":"lab-1-september-15.html","id":"normality-of-residuals","chapter":"1 Lab 1 â€” September 15","heading":"1.2.10.3 Normality of residuals","text":"also assumption residuals normally distributed. can checked \ncreating QQ-plot residuals overlaying QQ-line.Strong deviation QQ-line evidence violation normality assumption. \ncase plot, reason believe residuals \nnormally distributed.","code":"\nqqnorm(resid(rock_intercept_model))\nqqline(resid(rock_intercept_model), col=\"darkred\", lwd=2)"},{"path":"lab-3-september-29.html","id":"lab-3-september-29","chapter":"2 Lab 3 â€” September 29","heading":"2 Lab 3 â€” September 29","text":"","code":""},{"path":"lab-3-september-29.html","id":"some-modern-data-science-packages","chapter":"2 Lab 3 â€” September 29","heading":"2.1 Some modern data science packages","text":"tidyverse collection packages modern data\nscience. Tidymodels another collection packages \nextends tidyverse modelling. labs, \nusing entire tidyverse tidymodels. now, recommend installing \ncomponent packages.tibble, dplyr,\nggplot2, broom. \npackages can installed using:successfully installing packages, can load using:Additionally, ggplot2 comes various plotting themes (examples\ncan found ). \ndefault, plotting background grey prefer white background also call:tibble package provides us usage \"tibbles\" \nextension base-R data frame. many features, mostly using tibbles\npretty-printing features \nfact integrate well packages.dplyr package provides us tools data wrangling \nbase-R equivalents can feel bit clunky. Examples data wrangling include: keeping/removing\nobservations based presence/absence one conditions, creating additional variables\ndata set, condensing existing data summaries.ggplot2 package alternative base-R graphics\nfunctions \"smarter\" exist base-R. example, previous\nlab, mentioned plotting lines base-R, points needed sorted left right\nsince lines() function joins points order appear. Without sorting, \nresulting line appear jagged unevenly coloured certain areas. \nggplot2, separate functions joining points \nappear data joining points order left right.broom package contains tools cleaning model output \nextracting model data diagnostic information ready visualisation \nggplot2.","code":"\ninstall.packages(\"tibble\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"broom\")\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(broom)\ntheme_set(theme_bw())"},{"path":"lab-3-september-29.html","id":"back-to-simple-linear-regression","chapter":"2 Lab 3 â€” September 29","heading":"2.2 Back to simple linear regression","text":"","code":""},{"path":"lab-3-september-29.html","id":"the-mpg-data-set","chapter":"2 Lab 3 â€” September 29","heading":"2.2.1 The mpg data set","text":"Consider fuel economy data set found ggplot2 package.\ncan load data set calling:Suppose interested fitting simple linear regression model highway miles per gallon\n(hwy) response variable city miles per gallon (cty) predictor variable.\nfitting model, check see linear relationship \nchosen variables.","code":"\ndata(mpg, package=\"ggplot2\")"},{"path":"lab-3-september-29.html","id":"check-for-a-linear-relationship","chapter":"2 Lab 3 â€” September 29","heading":"2.2.2 Check for a linear relationship","text":"create plot, use function ggplot.call ggplot can thought initialisation drawing canvasThe first argument ggplot data set, mpgThe second argument ggplot mapping, .e. \"variables contain data \ninterested plotting?\"mapping created supplying names variables found within data set \naesthetics function, aes().Finally, can add (\"plus\" symbol) layer points geom_point() inherit\naesthetics supplied initialisation canvasIn words, specified within geom_point(), points drawn positions\nx=cty y=hwyFrom plot , appears linear relationship two variables. can\nbegin fitting linear model.","code":"\nggplot(mpg, aes(x=cty, y=hwy))+\n  geom_point()"},{"path":"lab-3-september-29.html","id":"fitting-the-linear-model","chapter":"2 Lab 3 â€” September 29","heading":"2.2.3 Fitting the linear model","text":"regression output, \\(\\widehat{\\beta}_{0} = 0.892\\) \n\\(\\widehat{\\beta}_{1} = 1.337\\). equation fitted line :\\[\\widehat{Y}_{} \\,=\\, 0.892 \\,+\\, 1.337X_{}\\]can interpret \\(\\widehat{\\beta}_{0} = 0.892\\) mean highway miles per gallon city\nmiles per gallon value zero. However, really make sense context \ndata since car travels zero city miles gallon gas.can interpret \\(\\widehat{\\beta}_{1} = 1.337\\) mean change highway miles per gallon per\nunit increase city miles per gallon. , 1 mpg increase city mpg, expect 1.337\nmpg increase highway mpg.","code":"\nlm_miles <- lm(hwy ~ cty, data=mpg)\n\nsummary(lm_miles)## \n## Call:\n## lm(formula = hwy ~ cty, data = mpg)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.3408 -1.2790  0.0214  1.0338  4.0461 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.89204    0.46895   1.902   0.0584 .  \n## cty          1.33746    0.02697  49.585   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.752 on 232 degrees of freedom\n## Multiple R-squared:  0.9138, Adjusted R-squared:  0.9134 \n## F-statistic:  2459 on 1 and 232 DF,  p-value: < 2.2e-16"},{"path":"lab-3-september-29.html","id":"visualising-the-fit","chapter":"2 Lab 3 â€” September 29","heading":"2.2.4 Visualising the fit","text":"ggplot2 broom\nshine! first use broom::augment() linear model extract data used \nfit model, along additional diagnostics (residuals). Note \nbroom::augment() generic function. pass lm object broom::augment(), \nhood, actually calls specific broom::augment.lm() function.first two columns lm_miles_aug, recovered data initially passed\nlm(). .fitted column contains fitted values model .resid column\ncontains raw residuals.plot wish build use newly created lm_miles_aug data set \nfollowing features:base plot scatterplot points located x=cty y=hwyThe fitted line drawn top points located x=cty y=.fittedSince points lines wish draw longer share common y-aesthetic, may \nwant declare y-aesthetic initialisation canvas, instead, declare \ny-aesthetic individual layers.Notice building plot, sort points! default, geom_line()\nconnects points left right. added additional arguments geom_line() outside\naesthetics since values depend data:colour controls colour line (can also use color)lwd controls width line (can also use linewidth)alpha controls transparency lineTo see construction plot \"addition layers\" can runfollowed byfollowed ","code":"\nlm_miles_aug <- augment(lm_miles)\n\nlm_miles_aug## # A tibble: 234 x 8\n##      hwy   cty .fitted .resid    .hat .sigma     .cooksd .std.resid\n##    <int> <int>   <dbl>  <dbl>   <dbl>  <dbl>       <dbl>      <dbl>\n##  1    29    18    25.0 4.03   0.00458   1.74 0.0123          2.31  \n##  2    29    21    29.0 0.0214 0.00834   1.76 0.000000632     0.0123\n##  3    31    20    27.6 3.36   0.00661   1.74 0.0123          1.92  \n##  4    30    21    29.0 1.02   0.00834   1.75 0.00144         0.585 \n##  5    26    16    22.3 3.71   0.00445   1.74 0.0101          2.12  \n##  6    26    18    25.0 1.03   0.00458   1.75 0.000805        0.591 \n##  7    27    18    25.0 2.03   0.00458   1.75 0.00311         1.16  \n##  8    26    18    25.0 1.03   0.00458   1.75 0.000805        0.591 \n##  9    25    16    22.3 2.71   0.00445   1.75 0.00536         1.55  \n## 10    28    20    27.6 0.359  0.00661   1.76 0.000140        0.205 \n## # ... with 224 more rows\n\nggplot(lm_miles_aug, aes(x=cty))+\n  geom_point(aes(y=hwy))+\n  geom_line(aes(y=.fitted), colour=\"#3366FF\", lwd=1.5, alpha=0.6)\nggplot(lm_miles_aug, aes(x=cty))\nggplot(lm_miles_aug, aes(x=cty))+\n  geom_point(aes(y=hwy))\nggplot(lm_miles_aug, aes(x=cty))+\n  geom_point(aes(y=hwy))+\n  geom_line(aes(y=.fitted), colour=\"#3366FF\", lwd=1.5, alpha=0.6)"},{"path":"lab-3-september-29.html","id":"confidence-and-prediction-intervals","chapter":"2 Lab 3 â€” September 29","heading":"2.2.5 Confidence and prediction intervals","text":"","code":""},{"path":"lab-3-september-29.html","id":"confidence-intervals","chapter":"2 Lab 3 â€” September 29","heading":"2.2.5.1 Confidence intervals","text":"Suppose interested finding point estimate 95% confidence interval mean\nhighway miles per gallon vehicles city miles per gallon value 20. can obtain \nvalues using predict() function. predict() another example generic function. \npass lm object predict(), actually calls specific predict.lm() function.\nsee need get point estimate confidence interval, pull associated\ndocumentation.need supply predict():linear model objectA data frame containing variables common linear model, values upon predict\nNote model predictor variable cty, supply\nnewdata = data.frame(x=20), must supply newdata = data.frame(cty=20)\nNote model predictor variable cty, supply\nnewdata = data.frame(x=20), must supply newdata = data.frame(cty=20)want confidence interval, specify interval = \"confidence\"default confidence level 0.95From output, point estimate 27.641. lower bound 95% confidence interval\n27.360 upper bound 27.922. says 95% confidence, vehicles \ncity miles per gallon value 20, mean highway miles per gallon range 27.360 \n27.922.","code":"\n?predict.lm\npredict(\n  lm_miles,\n  newdata = data.frame(cty=20),\n  interval = \"confidence\"\n)##        fit      lwr      upr\n## 1 27.64115 27.36044 27.92187"},{"path":"lab-3-september-29.html","id":"prediction-intervals","chapter":"2 Lab 3 â€” September 29","heading":"2.2.5.2 Prediction intervals","text":"Suppose now interested finding point estimate 95% prediction interval \nhighway miles per gallon vehicle city miles per gallon 20. can obtain \nvalues manner similar previous example:output, point estimate 27.641. lower bound 95% prediction interval\n24.177 upper bound 31.105. says 95% confidence, vehicle city\nmiles per gallon value 20 highway miles per gallon value 24.177 31.105.","code":"\npredict(\n  lm_miles,\n  newdata = data.frame(cty=20),\n  interval = \"prediction\"\n)##        fit      lwr      upr\n## 1 27.64115 24.17733 31.10498"},{"path":"lab-3-september-29.html","id":"spot-the-differences","chapter":"2 Lab 3 â€” September 29","heading":"2.2.5.3 Spot the differences","text":"point estimates cases identical. However, lower upper bounds \nconfidence prediction intervals different. unsurprising since formulas used \ncompute bounds two intervals different.noted , general, fixed confidence level, prediction interval \nwider corresponding confidence interval.","code":""},{"path":"lab-3-september-29.html","id":"correlations","chapter":"2 Lab 3 â€” September 29","heading":"2.2.6 Correlations","text":"Using previously obtained augmented model data (lm_miles_aug), can easily obtain \ncorrelations \\(X_{}\\) \\(Y_{}\\),\\(Y_{}\\) \\(\\widehat{Y}_{}\\),\\(X_{}\\) \\(\\widehat{Y}_{}\\),compare among , compare model's coefficient determination,\n\\(R^{2}\\). Correlations obtained using cor() function. also use summarise()\nfunction dplyr package help us obtain correlations\nstaying context augmented data can reference variables using bare\nnames (.e. need use $ reference variables).Note also :values mathematically related coincidence? ðŸ˜² (See assignment 2\nquestion 3).","code":"\nsummarise(\n  lm_miles_aug,\n  corr_x_y = cor(cty, hwy),\n  corr_y_fitted = cor(hwy, .fitted),\n  corr_x_fitted = cor(cty, .fitted)\n)## # A tibble: 1 x 3\n##   corr_x_y corr_y_fitted corr_x_fitted\n##      <dbl>         <dbl>         <dbl>\n## 1    0.956         0.956             1\n\nsummarise(\n  lm_miles_aug,\n  corr_x_y_squared = cor(cty, hwy)^2,\n  corr_y_fitted_squared = cor(hwy, .fitted)^2,\n  corr_x_fitted_squared = cor(cty, .fitted)^2,\n  r.squared = summary(lm_miles)$r.squared\n)## # A tibble: 1 x 4\n##   corr_x_y_squared corr_y_fitted_squared corr_x_fitted_squared r.squared\n##              <dbl>                 <dbl>                 <dbl>     <dbl>\n## 1            0.914                 0.914                     1     0.914\n"},{"path":"lab-3-september-29.html","id":"simple-linear-regression-with-transformations","chapter":"2 Lab 3 â€” September 29","heading":"2.3 Simple linear regression with transformations","text":"applying transformations variables linear models, two options:Create new variable data set applies transformation, orApply transformation formula specificationTo observe differences, build two models using methods. Using mpg data set\n, let us fit model response engine displacement predictor \n(natural) log highway miles per gallon.","code":""},{"path":"lab-3-september-29.html","id":"creating-a-new-variable-in-your-data-set","chapter":"2 Lab 3 â€” September 29","heading":"2.3.1 Creating a new variable in your data set","text":"dplyr loaded, can easily create new variables data set using mutate()\nfunction. first argument mutate() data set. Additional arguments name-value\npairs variables want create.model created usual:","code":"\nmpg2 <- mutate(\n  mpg,\n  log_hwy = log(hwy)\n)\nlm_displ1 <- lm(displ ~ log_hwy, data=mpg2)"},{"path":"lab-3-september-29.html","id":"applying-the-transformation-in-the-formula-specification","chapter":"2 Lab 3 â€” September 29","heading":"2.3.2 Applying the transformation in the formula specification","text":"can apply transformations formula specification wish create new\nvariable ahead time. model created using:","code":"\nlm_displ2 <- lm(displ ~ log(hwy), data=mpg2)"},{"path":"lab-3-september-29.html","id":"comparing-our-two-models","chapter":"2 Lab 3 â€” September 29","heading":"2.3.3 Comparing our two models","text":"models identical! try predict engine displacement vehicle \nhighway miles per gallon value 29.predicted values different! ?models use variables created ahead time, value(s) supplied predict upon\nmust scale predictor values used fitting model.\nwords, supplying value 29 first model, predicting engine displacement\nvehicle log highway miles per gallon value 29! order properly predict \nengine displacement, must instead supply value log(29).models transformations applied formula specification, can supply values \noriginal scale transformation applied us calculating predicted value.","code":"\nsummary(lm_displ1)## \n## Call:\n## lm(formula = displ ~ log_hwy, data = mpg2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.2992 -0.5548 -0.1148  0.4252  3.7446 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  15.4145     0.6496   23.73   <2e-16 ***\n## log_hwy      -3.8260     0.2074  -18.45   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8243 on 232 degrees of freedom\n## Multiple R-squared:  0.5946, Adjusted R-squared:  0.5929 \n## F-statistic: 340.3 on 1 and 232 DF,  p-value: < 2.2e-16\nsummary(lm_displ2)## \n## Call:\n## lm(formula = displ ~ log(hwy), data = mpg2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.2992 -0.5548 -0.1148  0.4252  3.7446 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  15.4145     0.6496   23.73   <2e-16 ***\n## log(hwy)     -3.8260     0.2074  -18.45   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8243 on 232 degrees of freedom\n## Multiple R-squared:  0.5946, Adjusted R-squared:  0.5929 \n## F-statistic: 340.3 on 1 and 232 DF,  p-value: < 2.2e-16\npredict(\n  lm_displ1,\n  newdata = data.frame(log_hwy=29)\n)##         1 \n## -95.53829\npredict(\n  lm_displ2,\n  newdata = data.frame(hwy=29)\n)##       1 \n## 2.53138\npredict(\n  lm_displ1,\n  newdata = data.frame(log_hwy=log(29))\n)##       1 \n## 2.53138"},{"path":"lab-3-september-29.html","id":"warning","chapter":"2 Lab 3 â€” September 29","heading":"2.3.4 Warning â—","text":"operations applied formula specification treated mathematical\noperations. example, + formula specification means add variable saw\nsection 1.2.8, - means remove variable. addition, ^ symbol usually use\nexponentiation different meaning used formula specification. example, \nwanted fit model using square highway miles per gallon predictor, \nwrite:Instead, must either create variable ahead time wrap exponentiation ():operation wrapped () formula specification, means treat literal\nmathematical operator rather formula operator.","code":"\nlm(displ ~ hwy^2, data=mpg2)\nlm(displ ~ I(hwy^2), data=mpg2)"},{"path":"lab-3-september-29.html","id":"another-example","chapter":"2 Lab 3 â€” September 29","heading":"2.4 Another example","text":"sake illustration, suppose wish fit model response variable \nlog highway miles per gallon plus log city miles per gallon predictor\nvariable square engine displacement. create response variable ahead\ntime apply transformation predictor formula specification.model fitted using:equation fitted line :\\[\\widehat{Y}_{} \\,=\\, 6.445 \\,-\\, 0.039X_{}^{2}\\]\\(\\widehat{y}_{}\\) sum log highway miles per gallon log city miles per\ngallon. Due model constructed, wanted predict sum log highway\nmiles per gallon log city highway miles per gallon particular vehicle, supply\ndisplacement values -model square .","code":"\nmpg2 <- mutate(\n  mpg2,\n  mpg_sum = log(hwy) + log(cty)\n)\nlm_log_miles <- lm(mpg_sum ~ I(displ^2), data=mpg2)\n\nsummary(lm_log_miles)## \n## Call:\n## lm(formula = mpg_sum ~ I(displ^2), data = mpg2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.91040 -0.18887  0.00019  0.16808  1.33149 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  6.444537   0.037335  172.61   <2e-16 ***\n## I(displ^2)  -0.038570   0.002214  -17.42   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3324 on 232 degrees of freedom\n## Multiple R-squared:  0.5668, Adjusted R-squared:  0.5649 \n## F-statistic: 303.6 on 1 and 232 DF,  p-value: < 2.2e-16"},{"path":"lab-3-september-29.html","id":"visualise-the-fit","chapter":"2 Lab 3 â€” September 29","heading":"2.4.1 Visualise the fit","text":"can visualise fit using procedure first augmenting linear model.two things note :name column predictor values violates usual naming conventions containing\nbrackets caret name wrapped backticksThe values column squared displacement valuesThis can verified comparing values lm_log_miles_aug values mpg2 â€”\nrows reordered way , example, first row lm_log_miles_aug\ncorresponds first row mpg2We can visualise fit passing augmented model data ggplot():Obviously, great fit goal example!","code":"\nlm_log_miles_aug <- augment(lm_log_miles)\n\nlm_log_miles_aug## # A tibble: 234 x 7\n##    mpg_sum `I(displ^2)` .fitted    .hat .sigma    .cooksd .std.resid\n##      <dbl>     <I<dbl>>   <dbl>   <dbl>  <dbl>      <dbl>      <dbl>\n##  1    6.26         3.24    6.32 0.00914  0.333 0.000162      -0.187 \n##  2    6.41         3.24    6.32 0.00914  0.333 0.000359       0.279 \n##  3    6.43         4       6.29 0.00846  0.333 0.000758       0.421 \n##  4    6.45         4       6.29 0.00846  0.333 0.000941       0.470 \n##  5    6.03         7.84    6.14 0.00580  0.333 0.000330      -0.336 \n##  6    6.15         7.84    6.14 0.00580  0.333 0.00000106     0.0191\n##  7    6.19         9.61    6.07 0.00502  0.333 0.000290       0.339 \n##  8    6.15         3.24    6.32 0.00914  0.333 0.00123       -0.517 \n##  9    5.99         3.24    6.32 0.00914  0.332 0.00454       -0.992 \n## 10    6.33         4       6.29 0.00846  0.333 0.0000553      0.114 \n## # ... with 224 more rows\n\nggplot(lm_log_miles_aug, aes(x=`I(displ^2)`))+\n  geom_point(aes(y=mpg_sum))+\n  geom_line(aes(y=.fitted), colour=\"#3366FF\", lwd=1.5, alpha=0.6)"},{"path":"lab-5-october-13.html","id":"lab-5-october-13","chapter":"3 Lab 5 â€” October 13","heading":"3 Lab 5 â€” October 13","text":"","code":""},{"path":"lab-5-october-13.html","id":"packages","chapter":"3 Lab 5 â€” October 13","heading":"3.1 Packages","text":"tidyr package another component package \ntidyverse contains functions cleaning \nreshaping data.palmerpenguins package used \npenguins data set multiple linear regression example today.using package ellipse drawing\nellipses (confidence regions). package contains functions drawing confidence regions\nassociated estimates obtained linear regression. Unlike code provided class \ndrawing confidence regions component confidence intervals, took raw data input,\nellipse::ellipse.lm() function allow pass linear model object instead. \nusing ellipse package, however need \ndraw lines corresponding component confidence intervals (though \ndifficult ).","code":"\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(ellipse)\nlibrary(palmerpenguins)\ntheme_set(theme_bw())"},{"path":"lab-5-october-13.html","id":"the-pipe","chapter":"3 Lab 5 â€” October 13","heading":"3.2 The pipe","text":"pipe operator, %>%, can found package magrittr,\ninstalled installed dplyr\npackage. dplyr loaded (calling library(dplyr)),\nmagrittr pipe also imported.pipe works taking result left passing first argument \nfunction right. pipe allows write cleaner code converting something like ::revealing underlying sequential logic workflow. Since pipe works taking \nresult left passing first argument function right, use \npipe wrangling data dplyr especially convenient \ndplyr data wrangling functions data frame argument \nfirst argument, cases, return data frame.example , first argument filter(), select(), mutate() data frame.\nevaluation, function also returns data frame. Notice result piping data\nframe functions, need actually specify going (since \ngoes first argument default) need create extra variables intermediate\nsteps.","code":"\nmutate(select(filter(mydata, var1 > 5), var2, var3), var4 = var2 + sqrt(var3))\nmydata %>%\n  filter(var1 > 5) %>%\n  select(var2, var3) %>%\n  mutate(var4 = var2 + sqrt(var3))"},{"path":"lab-5-october-13.html","id":"multiple-linear-regression-hypothesis-testing","chapter":"3 Lab 5 â€” October 13","heading":"3.3 Multiple linear regression â€” hypothesis testing","text":"","code":""},{"path":"lab-5-october-13.html","id":"the-penguins-data","chapter":"3 Lab 5 â€” October 13","heading":"3.3.1 The penguins data","text":"Let us load penguins data set. variables interested working \ncontain missing values. Immediately loading data, can pass drop_na()\nfunction (tidyr) drop rows (observations) \ncontain missing values.addition, also rename variables shorten code need \ntype. can done piping data rename() function supplying name pairs\nformat new_name = old_name.","code":"\npenguins <- palmerpenguins::penguins %>%\n  drop_na() %>%\n  rename(\n    bill_length = bill_length_mm,\n    bill_depth = bill_depth_mm,\n    flipper_length = flipper_length_mm,\n    body_mass = body_mass_g\n  )\n\npenguins## # A tibble: 333 x 8\n##    species island    bill_length bill_depth flipper_length body_mass sex    year\n##    <fct>   <fct>           <dbl>      <dbl>          <int>     <int> <fct> <int>\n##  1 Adelie  Torgersen        39.1       18.7            181      3750 male   2007\n##  2 Adelie  Torgersen        39.5       17.4            186      3800 fema~  2007\n##  3 Adelie  Torgersen        40.3       18              195      3250 fema~  2007\n##  4 Adelie  Torgersen        36.7       19.3            193      3450 fema~  2007\n##  5 Adelie  Torgersen        39.3       20.6            190      3650 male   2007\n##  6 Adelie  Torgersen        38.9       17.8            181      3625 fema~  2007\n##  7 Adelie  Torgersen        39.2       19.6            195      4675 male   2007\n##  8 Adelie  Torgersen        41.1       17.6            182      3200 fema~  2007\n##  9 Adelie  Torgersen        38.6       21.2            191      3800 male   2007\n## 10 Adelie  Torgersen        34.6       21.1            198      4400 male   2007\n## # ... with 323 more rows\n"},{"path":"lab-5-october-13.html","id":"subsetting-the-data","chapter":"3 Lab 5 â€” October 13","heading":"3.3.2 Subsetting the data","text":"data set contains penguins three species across three islands:following example, let us focus penguins species Adelie island\nBiscoe. can obtain subset penguins data using following code:(Note code , using two equal signs since making comparison \nsetting values).","code":"\npenguins %>%\n  distinct(species, island)## # A tibble: 5 x 2\n##   species   island   \n##   <fct>     <fct>    \n## 1 Adelie    Torgersen\n## 2 Adelie    Biscoe   \n## 3 Adelie    Dream    \n## 4 Gentoo    Biscoe   \n## 5 Chinstrap Dream\n\nadeliebiscoe <- penguins %>%\n  filter(species == \"Adelie\", island == \"Biscoe\")"},{"path":"lab-5-october-13.html","id":"visual-check-for-linear-relationship","chapter":"3 Lab 5 â€” October 13","heading":"3.3.3 Visual check for linear relationship","text":"Suppose interested fitting model using body mass response bill length, bill\ndepth, flipper length predictors. building model, verify \nlinear relationship response individual predictors.plots look okay. go ahead fit model.","code":"\nggplot(adeliebiscoe, aes(x=bill_length, y=body_mass))+\n  geom_point()\nggplot(adeliebiscoe, aes(x=bill_depth, y=body_mass))+\n  geom_point()\nggplot(adeliebiscoe, aes(x=flipper_length, y=body_mass))+\n  geom_point()"},{"path":"lab-5-october-13.html","id":"fit-the-model","chapter":"3 Lab 5 â€” October 13","heading":"3.3.4 Fit the model","text":"output, equation fitted line :\\[\\widehat{y}_{} \\,=\\, -6122 \\,+\\, 70.743x_{1,\\,} \\,+\\, 165.196x_{2,\\,} + 21.397x_{3,\\,}\\]:\\(\\widehat{y}_{}\\) predicted body mass\\(x_{1,\\,}\\) bill length\\(x_{2,\\,}\\) bill depth\\(x_{3,\\,}\\) flipper length","code":"\nad_lm <- lm(body_mass ~ bill_length + bill_depth + flipper_length, data=adeliebiscoe)\n\nsummary(ad_lm)## \n## Call:\n## lm(formula = body_mass ~ bill_length + bill_depth + flipper_length, \n##     data = adeliebiscoe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -836.50 -186.29   19.01  183.14  486.90 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    -6122.000   1341.697  -4.563 4.71e-05 ***\n## bill_length       70.743     21.445   3.299 0.002046 ** \n## bill_depth       165.196     43.802   3.771 0.000526 ***\n## flipper_length    21.397      7.262   2.947 0.005336 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 298.8 on 40 degrees of freedom\n## Multiple R-squared:  0.6508, Adjusted R-squared:  0.6246 \n## F-statistic: 24.85 on 3 and 40 DF,  p-value: 3.048e-09"},{"path":"lab-5-october-13.html","id":"fitting-a-model-without-an-intercept","chapter":"3 Lab 5 â€” October 13","heading":"3.3.5 Fitting a model without an intercept","text":"default, linear models fitted intercept. can fit linear model without \nintercept adding + 0 - 1 formula specification (see Details section \n?lm). make hypothesis tests next section bit interesting,\nlet us remove intercept model just fit overwrite .shown Lab 1, can fitting brand new model using lm():using update():Recall line code means (new) ad_lm constructed taking \nexisting ad_lm updating formula. response variable remains (represented \ndot left tilde), predictors remain (represented dot \nright tilde) - 1 denotes want intercept.take look new model.Interestingly, removing intercept model, adjusted R-squared value went \n0.6246 0.9905 ðŸ‘€.","code":"\nad_lm <- lm(body_mass ~ bill_length + bill_depth + flipper_length - 1, data=adeliebiscoe)\nad_lm <- update(ad_lm, formula = . ~ . - 1)\nsummary(ad_lm)## \n## Call:\n## lm(formula = body_mass ~ bill_length + bill_depth + flipper_length - \n##     1, data = adeliebiscoe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -737.69 -296.78   41.82  236.37  769.39 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)  \n## bill_length      58.741     25.922   2.266   0.0288 *\n## bill_depth      125.323     52.276   2.397   0.0212 *\n## flipper_length   -4.635      5.471  -0.847   0.4018  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 363.9 on 41 degrees of freedom\n## Multiple R-squared:  0.9912, Adjusted R-squared:  0.9905 \n## F-statistic:  1536 on 3 and 41 DF,  p-value: < 2.2e-16"},{"path":"lab-5-october-13.html","id":"anova-with-multiple-linear-regression-models","chapter":"3 Lab 5 â€” October 13","heading":"3.3.6 ANOVA with multiple linear regression models","text":"multiple linear regression, output resulting passing linear model anova() \nslightly different saw simple linear regression. simple linear regression,\nwrapping linear model anova() produced \"classic\" \\(F\\)-table: sum squares \nfirst row SSR, sum squares second row SSE, single\n\\(F\\)-value corresponding \\(p\\)-value.output, now three \\(F\\)-values three corresponding \\(p\\)-values! However, \ninterpretation slightly different. sum squares column represents sequential\nincrease SSR (decrease SSE) adding variable model came . \n:sum squares bill_length row increase SSR (decrease SSE) \nincluding bill_length model (base model intercept)sum squares bill_depth row increase SSR (decrease SSE) \nincluding bill_depth model already includes bill_lengthThe sum squares flipper_length row increase SSR (decrease SSE) \nincluding flipper_length model already includes bill_length bill_depthThe sum squares Residual row SSE model includes \nbill_length, bill_depth, flipper_lengthUsing output , wish perform hypothesis test check whether subset \nparameters different zero, need extra math. since using R,\nmath! can perform partial \\(F\\)-tests interested constructing\nreduced models (update() function becomes extremely handy).","code":"\nanova(ad_lm)## Analysis of Variance Table\n## \n## Response: body_mass\n##                Df    Sum Sq   Mean Sq   F value  Pr(>F)    \n## bill_length     1 609528281 609528281 4601.6610 < 2e-16 ***\n## bill_depth      1    683988    683988    5.1638 0.02837 *  \n## flipper_length  1     95065     95065    0.7177 0.40182    \n## Residuals      41   5430791    132458                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lab-5-october-13.html","id":"hypothesis-test-three-parameters","chapter":"3 Lab 5 â€” October 13","heading":"3.3.7 Hypothesis test: three parameters","text":"Suppose interested testing hypotheses:\\[H_{0}: \\beta_{1} \\,=\\, \\beta_{2} \\,=\\, \\beta_{3} \\,=\\, 0, \\quad\nH_{}: \\text{least one non-zero}\\]reduced model intercept-model passes origin.full model model predictors (ad_lm).test hypotheses, pass reduced full models anova():produces \\(F\\)-value 1535.8 numerator degrees freedom, 3, denominator degrees \nfreedom, 41. need use values since \\(p\\)-value also given. Using \nsignificance level \\(\\alpha = 0.05\\), since \\(p\\)-value less 0.05, reject null\nhypothesis conclude least one parameters non-zero , \nconsider constant \\((Y \\,=\\, 0)\\) model. important note result hypothesis\ntest tells us least one parameter non-zero, tell us .hypothesis test null hypothesis parameters equal zero often known\ntest model usefulness. fail reject null hypothesis, suggests \ncurrent model substantial improvement constant \\((Y \\,=\\, c)\\) model.","code":"\nad_lm_origin_only <- update(ad_lm, formula = . ~ 0)\nanova(ad_lm_origin_only, ad_lm)## Analysis of Variance Table\n## \n## Model 1: body_mass ~ 1 - 1\n## Model 2: body_mass ~ bill_length + bill_depth + flipper_length - 1\n##   Res.Df       RSS Df Sum of Sq      F    Pr(>F)    \n## 1     44 615738125                                  \n## 2     41   5430791  3 610307334 1535.8 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lab-5-october-13.html","id":"hypothesis-test-two-parameters","chapter":"3 Lab 5 â€” October 13","heading":"3.3.8 Hypothesis test: two parameters","text":"Suppose interested testing hypotheses:\\[H_{0}: \\beta_{2} \\,=\\, \\beta_{3} \\,=\\, 0, \\quad H_{}: \\text{least one non-zero}\\]reduced model model contains bill_length. can construct model using\nupdate() either specifying variables want:variables wantTo test hypotheses, pass reduced full models anova():produces \\(F\\)-value 2.9407 numerator degrees freedom, 2, denominator degrees\nfreedom, 41. Using usual significance level \\(\\alpha = 0.05\\), since \\(p\\)-value \ngreater 0.05, fail reject null hypothesis conclude insufficient\nevidence support claim least one \\(\\beta_{2}\\) \\(\\beta_{3}\\) non-zero.","code":"\n# Don't forget the no-intercept term!!\nad_lm1 <- update(ad_lm, formula = . ~ bill_length - 1)\nad_lm1 <- update(ad_lm, formula = . ~ . - bill_depth - flipper_length)\nanova(ad_lm1, ad_lm)## Analysis of Variance Table\n## \n## Model 1: body_mass ~ bill_length - 1\n## Model 2: body_mass ~ bill_length + bill_depth + flipper_length - 1\n##   Res.Df     RSS Df Sum of Sq      F  Pr(>F)  \n## 1     43 6209844                              \n## 2     41 5430791  2    779053 2.9407 0.06405 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lab-5-october-13.html","id":"hypothesis-test-one-parameter","chapter":"3 Lab 5 â€” October 13","heading":"3.3.9 Hypothesis test: one parameter","text":"Suppose interested testing hypotheses:\\[H_{0}: \\beta_{3} \\,=\\, 0, \\quad H_{}: \\beta_{3} \\,\\neq\\, 0\\]hypothesis can carried performing \\(F\\)-test \\(t\\)-test (\\(t\\)-test much\nsimpler). want perform \\(F\\)-test, take usual steps first constructing \nreduced model. reduced model model contains bill_length bill_depth. can\nconstruct model using update() removing flipper_length.test hypotheses, pass reduced full models anova():produces \\(F\\)-value 0.7177 numerator degrees freedom, 1, denominator degrees\nfreedom, 41. Using usual significance level \\(\\alpha = 0.05\\), since \\(p\\)-value \ngreater 0.05, fail reject null hypothesis conclude insufficient\nevidence support claim \\(\\beta_{3}\\) different zero.testing single parameter non-zero, done \\(t\\)-test, whose corresponding \\(p\\)-value\nread coefficient summary table.corresponding \\(p\\)-value \\(t\\)-test 0.402. Since \\(p\\)-value greater 0.05,\nfail reject null hypothesis .","code":"\nad_lm12 <- update(ad_lm, formula = . ~ . - flipper_length)\nanova(ad_lm12, ad_lm)## Analysis of Variance Table\n## \n## Model 1: body_mass ~ bill_length + bill_depth - 1\n## Model 2: body_mass ~ bill_length + bill_depth + flipper_length - 1\n##   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n## 1     42 5525856                           \n## 2     41 5430791  1     95065 0.7177 0.4018\ntidy(ad_lm)## # A tibble: 3 x 5\n##   term           estimate std.error statistic p.value\n##   <chr>             <dbl>     <dbl>     <dbl>   <dbl>\n## 1 bill_length       58.7      25.9      2.27   0.0288\n## 2 bill_depth       125.       52.3      2.40   0.0212\n## 3 flipper_length    -4.64      5.47    -0.847  0.402\n"},{"path":"lab-5-october-13.html","id":"caution","chapter":"3 Lab 5 â€” October 13","heading":"3.3.10 Caution â—","text":"Suppose output looked like:correct say:\\(p\\)-values tests var2 different zero var3 different zero \n0.4971 0.8846, respectively. Since \\(p\\)-values larger 0.05, fail reject \nnull hypothesis tests. Therefore can simultaneously remove var2 var3 \nmodel.\\(p\\)-value 0.497 corresponds testing var2 different zero, assuming var1 var3\nincluded model. Similarly, \\(p\\)-value 0.885 corresponds testing var3 different\nzero, assuming var1 var2 included model. Therefore proper procedure\nusing \\(t\\)-tests test single parameter non-zero, remove variable \nmodel (assuming failed reject null hypothesis), refit new model without variable,\nperform second \\(t\\)-test.","code":"## # A tibble: 3 x 5\n##   term  estimate std.error statistic p.value\n##   <chr> <chr>    <chr>     <chr>       <dbl>\n## 1 var1  ...      ...       ...        0.0001\n## 2 var2  ...      ...       ...        0.497 \n## 3 var3  ...      ...       ...        0.885\n"},{"path":"lab-5-october-13.html","id":"multiple-linear-regression-working-with-matrices-in-r","chapter":"3 Lab 5 â€” October 13","heading":"3.4 Multiple linear regression â€” working with matrices in R","text":"","code":""},{"path":"lab-5-october-13.html","id":"creating-a-matrix-in-r","chapter":"3 Lab 5 â€” October 13","heading":"3.4.1 Creating a matrix in R","text":"begin, recommend checking related documentation calling ?matrix. main\narguments interest :data: data matrixnrow: number rows matrix havencol: number columns matrix havebyrow: set TRUE, supplied data filled horizontally (row), otherwise \ndata filled vertically (column). default data filled vertically.illustrate data gets filled depending value byrow:","code":"\nnums <- 1:9\n\nmat1 <- matrix(nums, nrow=3, ncol=3)\nmat1##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nnums2 <- c(1:4, 9, 2, 7, 3, 6)\nmat2 <- matrix(nums2, nrow=3, ncol=3, byrow=TRUE)\nmat2##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    9    2\n## [3,]    7    3    6"},{"path":"lab-5-october-13.html","id":"matrix-operations","chapter":"3 Lab 5 â€” October 13","heading":"3.4.2 Matrix operations","text":"","code":""},{"path":"lab-5-october-13.html","id":"matrix-transpose","chapter":"3 Lab 5 â€” October 13","heading":"3.4.2.1 Matrix transpose","text":"obtain transpose matrix, use t():","code":"\nmat1##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nt(mat1)##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9"},{"path":"lab-5-october-13.html","id":"matrix-inverse","chapter":"3 Lab 5 â€” October 13","heading":"3.4.2.2 Matrix inverse","text":"obtain inverse matrix, use solve():solve() return error matrix invertible.","code":"\nmat2##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    9    2\n## [3,]    7    3    6\nsolve(mat2)##        [,1]   [,2]   [,3]\n## [1,] -0.384  0.024  0.184\n## [2,]  0.080  0.120 -0.080\n## [3,]  0.408 -0.088 -0.008"},{"path":"lab-5-october-13.html","id":"matrix-multiplication","chapter":"3 Lab 5 â€” October 13","heading":"3.4.2.3 Matrix multiplication","text":"multiply two matrices together, use %*%:use usual multiplication operator perform element-wise\nmultiplication, rather matrix multiplication:","code":"\nmat1 %*% mat2##      [,1] [,2] [,3]\n## [1,]   66   59   53\n## [2,]   78   73   64\n## [3,]   90   87   75\nmat1##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nmat2##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    9    2\n## [3,]    7    3    6\nmat1 * mat2##      [,1] [,2] [,3]\n## [1,]    1    8   21\n## [2,]    8   45   16\n## [3,]   21   18   54"},{"path":"lab-5-october-13.html","id":"example-question-k-page-174-selected-parts","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3 Example: Question K, page 174 (selected parts)","text":"given data, can start constructing matrices.forget column ones design matrix corresponds intercept! also\nsupply column names design matrix keep track columns \nresulting coefficient estimates names.supplying names matrix construction, list length 2 must supplied\nspecifying row column names respectively. Since need rownames supply NULL \nfirst element list. Note list length 2: NULL one element, \nc(\"Intercept\", \"X1\", \"X2\") another element. alternative build matrix names\nset names afterward using:","code":"\nY <- matrix(c(7.2, 8.1, 9.8, 12.3, 12.9), nrow=5, ncol=1)\nY##      [,1]\n## [1,]  7.2\n## [2,]  8.1\n## [3,]  9.8\n## [4,] 12.3\n## [5,] 12.9\nX <- matrix(\n  c(rep(1, 5), -1, -1, 0, 1, 1, -1, 0, 0, 0, 1),\n  nrow=5, ncol=3,\n  dimnames = list(NULL, c(\"Intercept\", \"X1\", \"X2\"))\n)\nX##      Intercept X1 X2\n## [1,]         1 -1 -1\n## [2,]         1 -1  0\n## [3,]         1  0  0\n## [4,]         1  1  0\n## [5,]         1  1  1\nrownames(X) <- c(\"row1\", \"row2\", \"etc.\")\ncolnames(X) <- c(\"col1\", \"col2\", \"etc.\")"},{"path":"lab-5-october-13.html","id":"obtain-the-coefficient-estimates","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.1 Obtain the coefficient estimates","text":"can obtain coefficient estimates using equation:\\[\\widehat{\\beta} \\,=\\, (X^{T}X)^{-1}\\,X^{T}\\,Y\\]equation fitted line :\\[\\widehat{y}_{} \\,=\\, 10.06 \\,+\\, 2.10x_{1,\\,} \\,+\\, 0.75x_{2,\\,}\\]","code":"\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat##            [,1]\n## Intercept 10.06\n## X1         2.10\n## X2         0.75"},{"path":"lab-5-october-13.html","id":"compute-the-following-regression-sum-of-squares","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.2 Compute the following regression sum of squares","text":"","code":""},{"path":"lab-5-october-13.html","id":"i","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.2.1 (i)","text":"\\[SS(\\widehat{\\beta}_{0},\\, \\widehat{\\beta}_{1},\\, \\widehat{\\beta}_{2}) \\,=\\, \\widehat{\\beta}^{T}\\,X^{T}\\,Y\\]","code":"\nt(betahat) %*% t(X) %*% Y##         [,1]\n## [1,] 531.083"},{"path":"lab-5-october-13.html","id":"ii","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.2.2 (ii)","text":"\\[\\begin{align*}\nSS(\\widehat{\\beta}_{1},\\, \\widehat{\\beta}_{2} \\,|\\, \\widehat{\\beta}_{0})\n&= SS(\\widehat{\\beta}_{0},\\, \\widehat{\\beta}_{1},\\, \\widehat{\\beta}_{2}) \\,-\\, SS(\\widehat{\\beta}_{0})\\\\\n&= \\widehat{\\beta}^{T}\\,X^{T}\\,Y \\,-\\, \\frac{1}{n}\\left(\\sum_{}Y_{}\\right)^{2}\\\\\n&= \\widehat{\\beta}^{T}\\,X^{T}\\,Y \\,-\\, \\frac{1}{n}Y^{T}\\,\\mathbf{1}\\,\\mathbf{1}^{T}\\,Y\n\\end{align*}\\]","code":"\nones <- matrix(1, nrow=5)\nones##      [,1]\n## [1,]    1\n## [2,]    1\n## [3,]    1\n## [4,]    1\n## [5,]    1\n(t(betahat) %*% t(X) %*% Y) - (1/5)*(t(Y) %*% ones %*% t(ones) %*% Y)##        [,1]\n## [1,] 25.065"},{"path":"lab-5-october-13.html","id":"iii","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.2.3 (iii)","text":"\\[SS(\\widehat{\\beta}_{2} \\,|\\, \\widehat{\\beta}_{1},\\, \\widehat{\\beta}_{0}) \\,=\\, SS(\\widehat{\\beta}_{0},\\, \\widehat{\\beta}_{1},\\, \\widehat{\\beta}_{2})\n\\,-\\, SS(\\widehat{\\beta}_{0},\\,\\widehat{\\beta}_{1})\\]already first term. second term obtained fitting model without \\(X_{2}\\)\nvariable computing\\[SS(\\widehat{\\beta}_{0},\\, \\widehat{\\beta}_{1}) \\,=\\, \\widehat{\\beta}^{T}\\,X^{T}\\,Y\\]Therefore quantity\\[SS(\\widehat{\\beta}_{2} \\,|\\, \\widehat{\\beta}_{1},\\, \\widehat{\\beta}_{0}) \\,=\\, SS(\\widehat{\\beta}_{0},\\, \\widehat{\\beta}_{1},\\,\\widehat{\\beta}_{2})\n\\,-\\, SS(\\widehat{\\beta}_{0},\\,\\widehat{\\beta}_{1})\\]can computed :","code":"\nX_reduced <- X[, -3]\nX_reduced##      Intercept X1\n## [1,]         1 -1\n## [2,]         1 -1\n## [3,]         1  0\n## [4,]         1  1\n## [5,]         1  1\nbetahat_reduced <- solve(t(X_reduced) %*% X_reduced) %*% t(X_reduced) %*% Y\nbetahat_reduced##             [,1]\n## Intercept 10.060\n## X1         2.475\nt(betahat_reduced) %*% t(X_reduced) %*% Y##          [,1]\n## [1,] 530.5205\n(t(betahat) %*% t(X) %*% Y) - (t(betahat_reduced) %*% t(X_reduced) %*% Y)##        [,1]\n## [1,] 0.5625"},{"path":"lab-5-october-13.html","id":"obtain-the-residual-sum-of-squares","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.3 Obtain the residual sum of squares","text":"\\[SSE \\,=\\, Y^{T}\\,Y \\,-\\, \\widehat{\\beta}^{T}\\,X^{T}\\,Y\\]","code":"\nSSE <- (t(Y) %*% Y) - (t(betahat) %*% t(X) %*% Y)\nSSE##       [,1]\n## [1,] 0.107"},{"path":"lab-5-october-13.html","id":"obtain-an-estimate-for-error-term-variance","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.4 Obtain an estimate for error term variance","text":"\\[\\widehat{\\sigma}^{2} \\,=\\, S^{2} \\,=\\, MSE \\,=\\, \\frac{SSE}{\\text{df}_{SSE}}\\]degrees freedom SSE \\(n - p - 1\\) \\(n\\) number observations, \\(p\\)\nnumber non-intercept parameters estimated. , \\(n = 5\\) \\(p = 2\\). \\[\\text{df}_{SSE} \\,=\\, n - p - 1 \\,=\\, 5 - 2 - 1 \\,=\\, 2\\]Note: need wrap MSE .numeric() coerce scalar future calculations.\nOtherwise, R, value still treated 1x1 matrix matrix operations may fail due\nincompatible matrix dimensions.","code":"\nMSE <- SSE / 2\nMSE <- as.numeric(MSE)\nMSE## [1] 0.0535"},{"path":"lab-5-october-13.html","id":"obtain-the-variance-covariance-matrix-of-the-coefficient-estimates","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.5 Obtain the variance-covariance matrix of the coefficient estimates","text":"\\[\\textbf{Var}(\\widehat{\\beta}) \\,=\\, \\widehat{\\sigma}^{2}(X^{T}\\,X)^{-1}\\]variances along diagonal matrix. covariances -diagonal terms. \nwanted standard errors coefficient estimates need take square root\ndiagonal terms. can first extracting diagonal elements wrapping\nvcov_betahat diag(), wrapping sqrt().","code":"\nvcov_betahat <- MSE * solve(t(X) %*% X)\nvcov_betahat##           Intercept       X1       X2\n## Intercept    0.0107  0.00000  0.00000\n## X1           0.0000  0.02675 -0.02675\n## X2           0.0000 -0.02675  0.05350\nse_betahat <- sqrt(diag(vcov_betahat))\nse_betahat## Intercept        X1        X2 \n## 0.1034408 0.1635543 0.2313007"},{"path":"lab-5-october-13.html","id":"predict-the-mean-response-value-at-a-given-point","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.6 Predict the mean response value at a given point","text":"Find \\(\\widehat{Y}_{0}\\) point \\((X_{1,\\,0},\\, X_{2,\\,0}) \\,=\\, (0.5, 0)\\).\\[\\widehat{y}_{0} \\,=\\, x_{0}^{T}\\widehat{\\beta} \\,=\\, \\begin{bmatrix}1 &0.5 &0\\end{bmatrix}\n\\begin{bmatrix}10.06\\\\ 2.10\\\\ 0.75\\end{bmatrix}\\]","code":"\nx0 <- matrix(c(1, 0.5, 0), nrow=3, ncol=1)\nx0##      [,1]\n## [1,]  1.0\n## [2,]  0.5\n## [3,]  0.0\nt(x0) %*% betahat##       [,1]\n## [1,] 11.11"},{"path":"lab-5-october-13.html","id":"obtain-the-standard-error-of-the-mean-response-at-the-given-point","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.7 Obtain the standard error of the mean response at the given point","text":"\\[\\textbf{se}(\\widehat{Y}_{0}) \\,=\\, \\sqrt{\\widehat{\\sigma}^{2}x_{0}^{T}(X^{T}\\,X)^{-1}x_{0}}\\]","code":"\nsqrt(MSE * t(x0) %*% solve(t(X) %*% X) %*% x0)##           [,1]\n## [1,] 0.1318617"},{"path":"lab-5-october-13.html","id":"weighted-least-squares","chapter":"3 Lab 5 â€” October 13","heading":"3.4.3.8 Weighted least squares","text":"information given question, variance error term now\\[\\textbf{Var}(\\varepsilon) \\,=\\, \\sigma^{2}V\\]\\[V \\,=\\, \n\\begin{bmatrix}\n1 &0 &0 &0 &0\\\\\n0 &1 &0 &0 &0\\\\\n0 &0 &1/4 &0 &0\\\\\n0 &0 &0 &1 &0\\\\\n0 &0 &0 &0 &1\n\\end{bmatrix}\\]\\(V^{-1}\\) \"weight matrix\".new estimates computed using:\\[\\widehat{\\beta} \\,=\\, (X^{T}\\,V^{-1}\\,X)^{-1}\\,X^{T}\\,V^{-1}\\,Y\\]Recall inverse diagonal matrix still diagonal matrix diagonal terms\nreciprocated.","code":"\nV <- diag(c(1, 1, 1/4, 1, 1))\nV##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    0 0.00    0    0\n## [2,]    0    1 0.00    0    0\n## [3,]    0    0 0.25    0    0\n## [4,]    0    0 0.00    1    0\n## [5,]    0    0 0.00    0    1\nV_inv <- solve(V)\nV_inv##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    0    0    0    0\n## [2,]    0    1    0    0    0\n## [3,]    0    0    4    0    0\n## [4,]    0    0    0    1    0\n## [5,]    0    0    0    0    1\nbetahat_weighted <- solve(t(X) %*% V_inv %*% X) %*% t(X) %*% V_inv %*% Y\nbetahat_weighted##             [,1]\n## Intercept 9.9625\n## X1        2.1000\n## X2        0.7500"},{"path":"lab-5-october-13.html","id":"simple-linear-regression-simultaneous-confidence-regions","chapter":"3 Lab 5 â€” October 13","heading":"3.5 Simple linear regression â€” simultaneous confidence regions","text":"Let us return rock data set consider simple linear regression model area \nresponse variable peri predictor.Suppose interested constructing 90% confidence region \\((\\beta_{0}, \\beta_{1})\\) \nindividual 95% confidence intervals. can start obtaining path ellipse passing\nlinear model ellipse(). default confidence level ellipse 95% need \nmake slight adjustment value level parameter. addition, ellipse() returns \nmatrix coordinates path ellipse. Since passing coordinates \npath ellipse ggplot later, also convert data frame.values (Intercept) peri columns represent x- y-values, respectively,\npath ellipse. Note ellipse() another example generic function. \npassing linear model object (object class lm) ellipse(), calls specific\nellipse.lm().Now path ellipse, also need obtain point\n\\((\\widehat{\\beta}_{0},\\, \\widehat{\\beta}_{1})\\), individual 95% confidence intervals \n\\(\\beta_{0}\\) \\(\\beta_{1}\\). can obtain passing linear model tidy.lm() \nspecifying conf.int=TRUE obtain coefficient table corresponding 95% confidence\nintervals.can begin plotting, data reshaping. Recall plotting \nggplot(), need x-values one column y-values another column.need data frame estimate \\(\\beta_{0}\\) one column estimate \n\\(\\beta_{1}\\) another column.need data frame lower upper bounds confidence interval \\(\\beta_{0}\\)\nsingle column.need data frame lower upper bounds confidence interval \\(\\beta_{1}\\)\nsingle column.Now data \"shape\", can start building plot. plot constructed \ntaking advantage fact every geom_*() layer data argument. means\ncan use different data set layer plot. Since common data\nset common aesthetics shared layers, supply anything call\nggplot(). Try running code , layer layer, see plot constructed!Note points contained within ellipse values \\((\\beta_{0},\\,\\beta_{1})\\) data\nsuggest jointly reasonable parameters. Meanwhile, within rectangular region\nplausible values parameters considered individually.","code":"\ndata(rock)\n\nslr_rock <- lm(area ~ peri, data=rock)\nellipse_path <- slr_rock %>%\n  ellipse(level=0.90) %>%\n  as_tibble()\n\nellipse_path## # A tibble: 100 x 2\n##    `(Intercept)`  peri\n##            <dbl> <dbl>\n##  1         3305.  1.63\n##  2         3239.  1.65\n##  3         3174.  1.67\n##  4         3107.  1.69\n##  5         3041.  1.71\n##  6         2974.  1.73\n##  7         2908.  1.74\n##  8         2842.  1.76\n##  9         2777.  1.78\n## 10         2714.  1.79\n## # ... with 90 more rows\n\ncoef_table <- slr_rock %>%\n  tidy(conf.int=TRUE)\n\ncoef_table## # A tibble: 2 x 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n## 1 (Intercept)  3052.     477.         6.40 7.26e- 8  2092.     4012.  \n## 2 peri            1.54     0.157      9.81 7.51e-13     1.23      1.86\n\ncoef_point <- coef_table %>%\n  pivot_wider(id_cols=c(term, estimate), names_from=term, values_from=estimate)\n\ncoef_point## # A tibble: 1 x 2\n##   `(Intercept)`  peri\n##           <dbl> <dbl>\n## 1         3052.  1.54\n\nbeta0 <- coef_table %>%\n  slice(1) %>%\n  pivot_longer(cols=contains(\"conf\"))\n\nbeta0## # A tibble: 2 x 7\n##   term        estimate std.error statistic      p.value name      value\n##   <chr>          <dbl>     <dbl>     <dbl>        <dbl> <chr>     <dbl>\n## 1 (Intercept)    3052.      477.      6.40 0.0000000726 conf.low  2092.\n## 2 (Intercept)    3052.      477.      6.40 0.0000000726 conf.high 4012.\n\nbeta1 <- coef_table %>%\n  slice(2) %>%\n  pivot_longer(cols=contains(\"conf\"))\n\nbeta1## # A tibble: 2 x 7\n##   term  estimate std.error statistic  p.value name      value\n##   <chr>    <dbl>     <dbl>     <dbl>    <dbl> <chr>     <dbl>\n## 1 peri      1.54     0.157      9.81 7.51e-13 conf.low   1.23\n## 2 peri      1.54     0.157      9.81 7.51e-13 conf.high  1.86\n\nggplot()+\n  geom_vline(data=beta0, aes(xintercept=value), colour=\"#3366FF\", lty=2)+\n  geom_hline(data=beta1, aes(yintercept=value), colour=\"#3366FF\", lty=2)+\n  geom_point(data=coef_point, aes(x=`(Intercept)`, y=peri), colour=\"red\")+\n  geom_path(data=ellipse_path, aes(x=`(Intercept)`, y=peri))+\n  labs(\n    x=\"Intercept\", y=\"Slope\",\n    caption=\"Solid line represents the boundary of the 90% confidence region. Dashed lines represent\n    the boundaries of the 95% confidence intervals.\"\n  )"},{"path":"lab-7-november-10.html","id":"lab-7-november-10","chapter":"4 Lab 7 â€” November 10","heading":"4 Lab 7 â€” November 10","text":"","code":""},{"path":"lab-7-november-10.html","id":"packages-1","chapter":"4 Lab 7 â€” November 10","heading":"4.1 Packages","text":"readr another package \ntidyverse reading various types data. Many \nfunctions work similar base-R equivalents work bit better, smarter, read data\ntibble.using leaps package aid us \ncomputing possible models set predictors obtaining corresponding Mallows'\n\\(C_{p}\\) values models. leaps\npackage can also used best subset selection, forward selection, backward selection. \nlab, covering example backward selection, manually\nthroroughly illustrate process.","code":"\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(broom)\nlibrary(leaps)\ntheme_set(theme_bw())"},{"path":"lab-7-november-10.html","id":"weighted-linear-regression-via-lm","chapter":"4 Lab 7 â€” November 10","heading":"4.2 Weighted linear regression (via lm)","text":"Last time, saw compute weighted least squares estimates \\(\\beta\\) using matrices.\nNow, take look via lm() function. data use \nexample data Table 2.1 (page 51).entered data manually , double-check values correct. resulting\nregression equation \\(\\widehat{Y}_{} = 1.426 + 0.316X_{}\\) (page 51).Looks good!","code":"\ntable2.1 <- read.table(\"./data/table2.1_DS.txt\", header=TRUE) %>%\n  as_tibble()\n\nlm(Y ~ X, data=table2.1) %>%\n  coef()## (Intercept)           X \n##   1.4256449   0.3157857"},{"path":"lab-7-november-10.html","id":"problem-i","chapter":"4 Lab 7 â€” November 10","heading":"4.2.1 Problem (i)","text":"Suppose told 16th observation variance \\(16\\sigma^{2}\\) rather \\(\\sigma^{2}\\).\nObtain parameter estimates using weighted least squares.","code":""},{"path":"lab-7-november-10.html","id":"solution","chapter":"4 Lab 7 â€” November 10","heading":"4.2.2 Solution","text":"16th observation variance \\(16\\sigma^{2}\\), \\[\\textbf{Var}(\\varepsilon) \\,=\\, \\sigma^{2}V \\,=\\,\n\\sigma^{2}\\begin{bmatrix}1\\\\\n&1\\\\\n& &\\ddots\\\\\n& & &16\\\\\n& & & &\\ddots\\\\\n& & & & &1\\\\\n& & & & & &1\n\\end{bmatrix}\\]weight matrix, \\(V^{-1}\\), calculated inverse \\(V\\). know inverse \ndiagonal matrix matrix diagonal elements reciprocated. Thus \\[V^{-1} \\,=\\,\n\\begin{bmatrix}1\\\\\n&1\\\\\n& &\\ddots\\\\\n& & &1/16\\\\\n& & & &\\ddots\\\\\n& & & & &1\\\\\n& & & & & &1\n\\end{bmatrix}\\]weights supply lm() vector 15 ones, followed value \\(1/16\\),\nfollowed 7 ones. Instead typing number 1 15 times, can use rep() function\nrepeats value \\(n\\) times.Now can proceed building model.equation fitted line :\\[\\widehat{Y}_{} \\,=\\, 1.4421 \\,+\\, 0.3077X_{}\\]","code":"\nw1 <- c(rep(1, 15), 1/16, rep(1, 7))\nw1##  [1] 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000\n## [11] 1.0000 1.0000 1.0000 1.0000 1.0000 0.0625 1.0000 1.0000 1.0000 1.0000\n## [21] 1.0000 1.0000 1.0000\n# Check that it has the same length as the data (n=23)\nlength(w1)## [1] 23\n# Check that the value at position 16 is correct\nw1[16]## [1] 0.0625\nwm1 <- lm(Y ~ X, data=table2.1, weights=w1)\nsummary(wm1)## \n## Call:\n## lm(formula = Y ~ X, data = table2.1, weights = w1)\n## \n## Weighted Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.18059 -0.51519 -0.07287  0.25786  2.39631 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)   1.4421     0.5112   2.821   0.0102 *\n## X             0.3077     0.1155   2.663   0.0145 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8485 on 21 degrees of freedom\n## Multiple R-squared:  0.2525, Adjusted R-squared:  0.2169 \n## F-statistic: 7.094 on 1 and 21 DF,  p-value: 0.01454"},{"path":"lab-7-november-10.html","id":"problem-ii","chapter":"4 Lab 7 â€” November 10","heading":"4.2.3 Problem (ii)","text":"Suppose now told 16th observation variance \\(25\\sigma^{2}\\) rather \n\\(\\sigma^{2}\\). Obtain parameter estimates using weighted least squares.","code":""},{"path":"lab-7-november-10.html","id":"solution-1","chapter":"4 Lab 7 â€” November 10","heading":"4.2.4 Solution","text":"repeat process . Since variance 16th observation scaled \nfactor 25, weight \\(1/25\\).Building second model,equation fitted line :\\[\\widehat{Y}_{} \\,=\\, 1.4425 \\,+\\, 0.3075X_{}\\]can see changing weight 16th observation \\(1/16\\) \\(1/25\\):value \\(\\widehat{\\beta}_{0}\\) increased (much)value \\(\\widehat{\\beta}_{1}\\) decreased (much)differences apparent adjusting weights 23rd observation data set.\nHint: make scatterplot!First, add column indices data.can make scatterplot label point observation number.Observation 23 definitely stands rest data!","code":"\nw2 <- c(rep(1, 15), 1/25, rep(1, 7))\nw2##  [1] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n## [16] 0.04 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n# Check that it has the same length as the data (n=23)\nlength(w2)## [1] 23\n# Check that the value at position 16 is correct\nw2[16]## [1] 0.04\nwm2 <- lm(Y ~ X, data=table2.1, weights=w2)\nsummary(wm2)## \n## Call:\n## lm(formula = Y ~ X, data = table2.1, weights = w2)\n## \n## Weighted Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.17998 -0.51499 -0.07274  0.25852  2.39727 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)   1.4425     0.5112   2.822   0.0102 *\n## X             0.3075     0.1155   2.661   0.0146 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8483 on 21 degrees of freedom\n## Multiple R-squared:  0.2522, Adjusted R-squared:  0.2166 \n## F-statistic: 7.082 on 1 and 21 DF,  p-value: 0.01461\ntable2.1 <- table2.1 %>%\n  mutate(obs = 1:n(), .before = everything())\n\ntable2.1## # A tibble: 23 x 3\n##      obs     Y     X\n##    <int> <dbl> <dbl>\n##  1     1   2.3   1.3\n##  2     2   1.8   1.3\n##  3     3   2.8   2  \n##  4     4   1.5   2  \n##  5     5   2.2   2.7\n##  6     6   3.8   3.3\n##  7     7   1.8   3.3\n##  8     8   3.7   3.7\n##  9     9   1.7   3.7\n## 10    10   2.8   4  \n## # ... with 13 more rows\n\nggplot(table2.1, aes(x=X, y=Y, label=obs))+\n  geom_point(alpha=0.4)+\n  geom_text(hjust=\"outward\", vjust=\"outward\")"},{"path":"lab-7-november-10.html","id":"all-possible-models","chapter":"4 Lab 7 â€” November 10","heading":"4.3 All possible models","text":"consistent textbook, let us define \\(p\\) total number parameters model,\nincluding intercept. Suppose interested plotting Mallows' \\(C_{p}\\) \\(p\\) \npossible models.(Alternatively, consider plotting \\(C_{p}\\) \\(p+1\\) \\(p\\) number \nnon-intercept parameters).Returning rock data set, outline plan follows:compute possible models single line, employ leaps::regsubsets(). \nformula argument (x), specify formula area ~ .. means fix area response\nvariable, allow variables appearing data set added candidate\nmodel (need manually type names variables data set).\nalso set nbest=3 force computation models \\(p < 4\\).compute possible models single line, employ leaps::regsubsets(). \nformula argument (x), specify formula area ~ .. means fix area response\nvariable, allow variables appearing data set added candidate\nmodel (need manually type names variables data set).\nalso set nbest=3 force computation models \\(p < 4\\).pass result custom function derived broom::tidy.regsubsets()\nobtain required diagnostic information (including Mallows' \\(C_{p}\\)) tidy tibble. need\ncustom function since also want \\(SSE_{p}\\) model order calculate \\(MSE_{p}\\)\n(equivalently, \\(S^{2}_{p}\\)).pass result custom function derived broom::tidy.regsubsets()\nobtain required diagnostic information (including Mallows' \\(C_{p}\\)) tidy tibble. need\ncustom function since also want \\(SSE_{p}\\) model order calculate \\(MSE_{p}\\)\n(equivalently, \\(S^{2}_{p}\\)).can create column, \\(p\\), counts number variables included model\n(including intercept). involves code bit complicated, \nessentially summing number TRUEs appearing row (.e. performing \nhorizontal sum rather usual vertical sum).can create column, \\(p\\), counts number variables included model\n(including intercept). involves code bit complicated, \nessentially summing number TRUEs appearing row (.e. performing \nhorizontal sum rather usual vertical sum).compute mean square error, \\(S^{2}_{p}\\), model.compute mean square error, \\(S^{2}_{p}\\), model.drop columns using.drop columns using.Finally, can plot results.Finally, can plot results.","code":""},{"path":"lab-7-november-10.html","id":"a-custom-tidy-function","chapter":"4 Lab 7 â€” November 10","heading":"4.3.1 A custom tidy function","text":"can access source code used tidy objects class regsubsets calling\nbroom:::tidy.regsubsets (three colons).Consulting documentation ?leaps::regsubsets(), can create custom function also\nincludes SSE model.","code":"\nbroom:::tidy.regsubsets## function (x, ...) \n## {\n##     s <- summary(x)\n##     inclusions <- as_tibble(s$which)\n##     metrics <- with(s, tibble(r.squared = rsq, adj.r.squared = adjr2, \n##         BIC = bic, mallows_cp = cp))\n##     bind_cols(inclusions, metrics)\n## }\n## <bytecode: 0x00000000303a31d0>\n## <environment: namespace:broom>\ntidy.regsubsets2 <- function(x, ...) {\n  s <- summary(x)\n  inclusions <- as_tibble(s$which)\n  metrics <- with(s, tibble(r.squared = rsq, adj.r.squared = adjr2, \n                            BIC = bic, mallows_cp = cp, SSE_p = rss))\n  bind_cols(inclusions, metrics)\n}"},{"path":"lab-7-november-10.html","id":"getting-the-results","chapter":"4 Lab 7 â€” November 10","heading":"4.3.2 Getting the results","text":"Recall \\[\\widehat{\\sigma}^{2}_{p} \\,=\\, S^{2}_{p} \\,=\\, \\frac{SSE_{p}}{n-p}\\]\\(n\\) number observations data, \\(p\\) number parameters including\nintercept.choosing model, want model high adjusted \\(R^{2}\\), \\(C_{p} \\approx p\\), low\n\\(S^{2}_{p}\\), predictors possible. results , models 4 7 quite\nsimilar model 7 (full model) slightly better.","code":"\nn <- nrow(rock)\n\nall_models <- regsubsets(area ~ ., data=rock, method=\"exhaustive\", nbest=3) %>%\n  tidy.regsubsets2() %>%\n  rowwise() %>%\n  mutate(p = sum(c_across(where(is.logical)))) %>%\n  ungroup() %>%\n  mutate(s.squared_p = SSE_p / (n - p)) %>%\n  select(-c(r.squared, BIC, SSE_p))\n\nall_models## # A tibble: 7 x 8\n##   `(Intercept)` peri  shape perm  adj.r.squared mallows_cp     p s.squared_p\n##   <lgl>         <lgl> <lgl> <lgl>         <dbl>      <dbl> <int>       <dbl>\n## 1 TRUE          TRUE  FALSE FALSE        0.669       20.8      2    2380718.\n## 2 TRUE          FALSE FALSE TRUE         0.139      125.       2    6201808.\n## 3 TRUE          FALSE TRUE  FALSE        0.0122     150.       2    7115420.\n## 4 TRUE          TRUE  FALSE TRUE         0.764        3.20     3    1696625.\n## 5 TRUE          TRUE  TRUE  FALSE        0.701       15.4      3    2152972.\n## 6 TRUE          FALSE TRUE  TRUE         0.122      126.       3    6323336.\n## 7 TRUE          TRUE  TRUE  TRUE         0.765        4        4    1689242.\n"},{"path":"lab-7-november-10.html","id":"obtaining-the-values-for-the-intercept-only-model","chapter":"4 Lab 7 â€” November 10","heading":"4.3.3 Obtaining the values for the intercept-only model","text":"first fit intercept-model.formula Mallows' \\(C_{p}\\) given page 332 textbook:\\[C_{p} \\,=\\, \\frac{SSE_{p}}{S^{2}} \\,-\\, (n - 2p)\\]\\(S^{2}\\) estimate error term variance maximal model \\((p=4)\\). Note \nvalue \\(n\\) already initialised variable workspace. can extract \nvalue \\(S^{2}_{4}\\) previous results using:relationship \\[S^{2}_{1} \\,=\\, MSE_{1} \\,=\\, \\frac{SSE_{1}}{\\text{df}_{1}}\\]Rearranging gives\\[SSE_{1} \\,=\\, \\text{df}_{1} * S^{2}_{1}\\]Mimicking structure all_models, corresponding values intercept-model :Binding main output:","code":"\nrock_intercept_lm <- lm(area ~ 1, data=rock)\ns.squared_4 <- all_models %>%\n  slice_tail(n=1) %>%\n  pull(s.squared_p)\nintercept_lm_results <- tibble(\n  `(Intercept)` = TRUE,\n  peri = FALSE,\n  shape = FALSE,\n  perm = FALSE,\n  adj.r.squared = summary(rock_intercept_lm)$adj.r.squared,\n  s.squared_p = summary(rock_intercept_lm)$sigma^2,\n  p = summary(rock_intercept_lm)$df[1],\n  df = summary(rock_intercept_lm)$df[2],\n  mallows_cp = (s.squared_p * df) / s.squared_4 - (n - 2 * p)\n) %>%\n  select(where(is.logical), adj.r.squared, mallows_cp, p, s.squared_p)\n\nintercept_lm_results## # A tibble: 1 x 8\n##   `(Intercept)` peri  shape perm  adj.r.squared mallows_cp     p s.squared_p\n##   <lgl>         <lgl> <lgl> <lgl>         <dbl>      <dbl> <int>       <dbl>\n## 1 TRUE          FALSE FALSE FALSE             0       154.     1    7203045.\n\nall_models <- bind_rows(intercept_lm_results, all_models)\n\nall_models## # A tibble: 8 x 8\n##   `(Intercept)` peri  shape perm  adj.r.squared mallows_cp     p s.squared_p\n##   <lgl>         <lgl> <lgl> <lgl>         <dbl>      <dbl> <int>       <dbl>\n## 1 TRUE          FALSE FALSE FALSE        0          154.       1    7203045.\n## 2 TRUE          TRUE  FALSE FALSE        0.669       20.8      2    2380718.\n## 3 TRUE          FALSE FALSE TRUE         0.139      125.       2    6201808.\n## 4 TRUE          FALSE TRUE  FALSE        0.0122     150.       2    7115420.\n## 5 TRUE          TRUE  FALSE TRUE         0.764        3.20     3    1696625.\n## 6 TRUE          TRUE  TRUE  FALSE        0.701       15.4      3    2152972.\n## 7 TRUE          FALSE TRUE  TRUE         0.122      126.       3    6323336.\n## 8 TRUE          TRUE  TRUE  TRUE         0.765        4        4    1689242.\n"},{"path":"lab-7-november-10.html","id":"plotting-the-results","chapter":"4 Lab 7 â€” November 10","heading":"4.3.4 Plotting the results","text":"","code":"\nggplot(all_models, aes(x=p, y=mallows_cp))+\n  geom_point()+\n  labs(caption=\"p is the number of parameter estimates including the intercept\")"},{"path":"lab-7-november-10.html","id":"stepwise-selection","chapter":"4 Lab 7 â€” November 10","heading":"4.4 Stepwise selection","text":"revisit penguins data \npalmerpenguins, specifically subset \npenguins species Adelie island Biscoe.following, use:\\[\\alpha_{\\text{entry}} = 0.05, \\quad \\alpha_{\\text{exit}} = 0.05\\]","code":"\nadeliebiscoe <- palmerpenguins::penguins %>%\n  drop_na() %>%\n  filter(species == \"Adelie\", island == \"Biscoe\") %>%\n  rename(\n    bill_length = bill_length_mm,\n    bill_depth = bill_depth_mm,\n    flipper_length = flipper_length_mm,\n    body_mass = body_mass_g,\n  ) %>%\n  select(body_mass, bill_length, bill_depth, flipper_length)"},{"path":"lab-7-november-10.html","id":"building-the-base-model","chapter":"4 Lab 7 â€” November 10","heading":"4.4.1 Building the base model","text":"interested performing stepwise selection using body_mass response. recommended\nlecture notes, commence model building process including variable \ncorrelated body_mass.bill_depth correlated body_mass (remember look absolute value \ncorrelation).summary output, \\(t\\)-test bill_depth significant can continue considering\npredictors add model.","code":"\ncor(adeliebiscoe)##                body_mass bill_length bill_depth flipper_length\n## body_mass      1.0000000   0.6477493  0.6516595      0.5261972\n## bill_length    0.6477493   1.0000000  0.4681935      0.3366548\n## bill_depth     0.6516595   0.4681935  1.0000000      0.2727782\n## flipper_length 0.5261972   0.3366548  0.2727782      1.0000000\nstep1 <- lm(body_mass ~ bill_depth, data=adeliebiscoe)\n\nsummary(step1)## \n## Call:\n## lm(formula = body_mass ~ bill_depth, data = adeliebiscoe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -846.03 -227.10  -27.46  275.68  897.03 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -1201.76     883.92  -1.360    0.181    \n## bill_depth    267.35      48.02   5.568 1.66e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 374.3 on 42 degrees of freedom\n## Multiple R-squared:  0.4247, Adjusted R-squared:  0.411 \n## F-statistic:    31 on 1 and 42 DF,  p-value: 1.658e-06"},{"path":"lab-7-november-10.html","id":"adding-a-predictor-round-1","chapter":"4 Lab 7 â€” November 10","heading":"4.4.2 Adding a predictor, round 1","text":"Note can perform partial \\(F\\)-tests single variable additions using usual method\nbuilding candidate model calling anova() reduced full models.can become lot work step can become tedious many predictors \nconsider step. Instead, can perform partial \\(F\\)-tests single variable additions\nusing add1() function.second argument add1() scope, list predictors (using formula syntax)\neligible added main model. Notice results partial \\(F\\)-tests\nidentical constructed models individually (step2a step2b).variable gets added model one highest \\(F\\)-value. Since \n\\(F\\)-values numerator denominator degrees freedom, also equivalent \nadding variable smallest \\(p\\)-value associated partial \\(F\\)-test (long \nless \\(\\alpha_{\\text{entry}}\\)). Since \\(F\\)-test adding bill_length base\nmodel results smallest \\(p\\)-value less \\(\\alpha_{\\text{entry}}\\), bill_length\nadded base model.","code":"\nstep2a <- lm(body_mass ~ bill_depth + bill_length, data=adeliebiscoe)\nstep2b <- lm(body_mass ~ bill_depth + flipper_length, data=adeliebiscoe)\n\nanova(step1, step2a)## Analysis of Variance Table\n## \n## Model 1: body_mass ~ bill_depth\n## Model 2: body_mass ~ bill_depth + bill_length\n##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n## 1     42 5885163                                  \n## 2     41 4347044  1   1538118 14.507 0.0004594 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(step1, step2b)## Analysis of Variance Table\n## \n## Model 1: body_mass ~ bill_depth\n## Model 2: body_mass ~ bill_depth + flipper_length\n##   Res.Df     RSS Df Sum of Sq      F   Pr(>F)   \n## 1     42 5885163                                \n## 2     41 4543427  1   1341736 12.108 0.001205 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nadd1(step1, ~ . + bill_length + flipper_length, test=\"F\")## Single term additions\n## \n## Model:\n## body_mass ~ bill_depth\n##                Df Sum of Sq     RSS    AIC F value    Pr(>F)    \n## <none>                      5885163 523.37                      \n## bill_length     1   1538118 4347044 512.04  14.507 0.0004594 ***\n## flipper_length  1   1341736 4543427 513.98  12.108 0.0012047 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nstep2 <- update(step1, . ~ . + bill_length)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-1","chapter":"4 Lab 7 â€” November 10","heading":"4.4.3 Dropping a predictor, round 1","text":"Now check whether predictors can dropped. , can construct\nmultiple models differ single term deletions perform partial \\(F\\)-tests \ncalling anova() reduced full models. can also use equivalent add1() \nsingle term deletions, drop1().second argument drop1() scope (similar add1()), list predictors\n(using formula syntax) eligible dropped main model. , predictors\ncurrently model eligible dropped.Recall hypotheses associated dropping single variable:\\[H_{0}: \\beta_{1} = 0, \\quad H_{}: \\beta_{1} \\neq 0\\]\n\\[H_{0}: \\beta_{2} = 0, \\quad H_{}: \\beta_{2} \\neq 0\\]variable dropped model, need fail reject null hypothesis. \nlooking small \\(F\\)-values, equivalently, large \\(p\\)-values greater \n\\(\\alpha_{\\text{exit}}\\). Since , drop predictors \ncurrent model.","code":"\ndrop1(step2, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## body_mass ~ bill_depth + bill_length\n##             Df Sum of Sq     RSS    AIC F value    Pr(>F)    \n## <none>                   4347044 512.04                      \n## bill_depth   1   1590092 5937136 523.75  14.997 0.0003796 ***\n## bill_length  1   1538118 5885163 523.37  14.507 0.0004594 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lab-7-november-10.html","id":"adding-a-predictor-round-2","chapter":"4 Lab 7 â€” November 10","heading":"4.4.4 Adding a predictor, round 2","text":"stage, one variable left data set can added model.Since \\(p\\)-value \\(F\\)-test less \\(\\alpha_{\\text{entry}}\\), add flipper_length\nmain model.","code":"\nadd1(step2, ~ . + flipper_length, test=\"F\")## Single term additions\n## \n## Model:\n## body_mass ~ bill_depth + bill_length\n##                Df Sum of Sq     RSS    AIC F value   Pr(>F)   \n## <none>                      4347044 512.04                    \n## flipper_length  1    775322 3571722 505.39  8.6829 0.005336 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nstep3 <- update(step2, . ~ . + flipper_length)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-2","chapter":"4 Lab 7 â€” November 10","heading":"4.4.5 Dropping a predictor, round 2","text":", predictors considered dropping \\(p\\)-value greater\n\\(\\alpha_{\\text{exit}}\\). Since , drop predictors \nmodel.","code":"\ndrop1(step3, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## body_mass ~ bill_depth + bill_length + flipper_length\n##                Df Sum of Sq     RSS    AIC F value    Pr(>F)    \n## <none>                      3571722 505.39                      \n## bill_depth      1   1270094 4841816 516.78 14.2239 0.0005257 ***\n## bill_length     1    971705 4543427 513.98 10.8822 0.0020456 ** \n## flipper_length  1    775322 4347044 512.04  8.6829 0.0053359 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lab-7-november-10.html","id":"the-final-model","chapter":"4 Lab 7 â€” November 10","heading":"4.4.6 The final model","text":"final model :\\[\\widehat{Y}_{} \\,=\\, -6122.000 \\,+\\, 165.196X_{1,\\,} + 70.743X_{2,\\,} \\,+\\, 21.397X_{3,\\,}\\]Note adjusted R-squared value 0.6246, terrible, great. However,\nrecall last lab, removing intercept model result jump \nadjusted R-squared 0.6246 0.9905.","code":"\nsummary(step3)## \n## Call:\n## lm(formula = body_mass ~ bill_depth + bill_length + flipper_length, \n##     data = adeliebiscoe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -836.50 -186.29   19.01  183.14  486.90 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    -6122.000   1341.697  -4.563 4.71e-05 ***\n## bill_depth       165.196     43.802   3.771 0.000526 ***\n## bill_length       70.743     21.445   3.299 0.002046 ** \n## flipper_length    21.397      7.262   2.947 0.005336 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 298.8 on 40 degrees of freedom\n## Multiple R-squared:  0.6508, Adjusted R-squared:  0.6246 \n## F-statistic: 24.85 on 3 and 40 DF,  p-value: 3.048e-09"},{"path":"lab-7-november-10.html","id":"backward-selection","chapter":"4 Lab 7 â€” November 10","heading":"4.5 Backward selection","text":"backward selection, start full model step, consider dropping predictors.\nseen Adelie Biscoe data, performing \\(F\\)-test single term deletions \nmodel contained predictors, none eligible dropping. example, let us\nconsider \nScooby Doo data set,\nmany variables.","code":"\nscoobydoo <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-13/scoobydoo.csv\")\nnames(scoobydoo)##  [1] \"index\"                    \"series_name\"             \n##  [3] \"network\"                  \"season\"                  \n##  [5] \"title\"                    \"imdb\"                    \n##  [7] \"engagement\"               \"date_aired\"              \n##  [9] \"run_time\"                 \"format\"                  \n## [11] \"monster_name\"             \"monster_gender\"          \n## [13] \"monster_type\"             \"monster_subtype\"         \n## [15] \"monster_species\"          \"monster_real\"            \n## [17] \"monster_amount\"           \"caught_fred\"             \n## [19] \"caught_daphnie\"           \"caught_velma\"            \n## [21] \"caught_shaggy\"            \"caught_scooby\"           \n## [23] \"captured_fred\"            \"captured_daphnie\"        \n## [25] \"captured_velma\"           \"captured_shaggy\"         \n## [27] \"captured_scooby\"          \"unmask_fred\"             \n## [29] \"unmask_daphnie\"           \"unmask_velma\"            \n## [31] \"unmask_shaggy\"            \"unmask_scooby\"           \n## [33] \"snack_fred\"               \"snack_daphnie\"           \n## [35] \"snack_velma\"              \"snack_shaggy\"            \n## [37] \"snack_scooby\"             \"unmask_other\"            \n## [39] \"caught_other\"             \"caught_not\"              \n## [41] \"trap_work_first\"          \"setting_terrain\"         \n## [43] \"setting_country_state\"    \"suspects_amount\"         \n## [45] \"non_suspect\"              \"arrested\"                \n## [47] \"culprit_name\"             \"culprit_gender\"          \n## [49] \"culprit_amount\"           \"motive\"                  \n## [51] \"if_it_wasnt_for\"          \"and_that\"                \n## [53] \"door_gag\"                 \"number_of_snacks\"        \n## [55] \"split_up\"                 \"another_mystery\"         \n## [57] \"set_a_trap\"               \"jeepers\"                 \n## [59] \"jinkies\"                  \"my_glasses\"              \n## [61] \"just_about_wrapped_up\"    \"zoinks\"                  \n## [63] \"groovy\"                   \"scooby_doo_where_are_you\"\n## [65] \"rooby_rooby_roo\"          \"batman\"                  \n## [67] \"scooby_dum\"               \"scrappy_doo\"             \n## [69] \"hex_girls\"                \"blue_falcon\"             \n## [71] \"fred_va\"                  \"daphnie_va\"              \n## [73] \"velma_va\"                 \"shaggy_va\"               \n## [75] \"scooby_va\""},{"path":"lab-7-november-10.html","id":"data-prep","chapter":"4 Lab 7 â€” November 10","heading":"4.5.1 Data prep","text":"example, keep subset variables. addition, variables\ntype numeric read strings due presence string \"NULL\"\n(note string \"NULL\" NA).Therefore, want remove episodes contain \"NULL\" values selected columns \ncoerce remaining values numeric.line containing filter(), filtering character-type columns retain\nepisodes contain \"NULL\" values. Specifying filtering condition way\nallows us less typing. Otherwise need type variables interest:Note portion ~ . != \"NULL\" lambda function / anonymous function, mathematical\nformula, short-hand equivalent :Now ready start building models. interested creating model predicts\nIMDB score. performing backward selection, start model containing \npredictors. following example, use \\(\\alpha_{\\text{exit}} = 0.05\\).","code":"\nscoobydoo %>%\n  count(jinkies)## # A tibble: 14 x 2\n##    jinkies     n\n##    <chr>   <int>\n##  1 0         176\n##  2 1          92\n##  3 10          1\n##  4 11          1\n##  5 13          1\n##  6 2          53\n##  7 3          27\n##  8 4          11\n##  9 5           5\n## 10 6          10\n## 11 7           4\n## 12 8           3\n## 13 9           1\n## 14 NULL      218\n\nscoobydoo <- scoobydoo %>%\n  select(\n    imdb, engagement, run_time, monster_amount, suspects_amount, culprit_amount,\n    jeepers, jinkies, my_glasses, zoinks\n  ) %>%\n  filter(across(where(is.character), ~ . != \"NULL\")) %>%\n  mutate(across(where(is.character), as.numeric))\nfilter(imdb != \"NULL\", jeepers != \"NULL\", jinkies != \"NULL\", etc.)\nfunction(x) {\n  x != \"NULL\"\n}\nscoobydoo## # A tibble: 369 x 10\n##     imdb engagement run_time monster_amount suspects_amount culprit_amount\n##    <dbl>      <dbl>    <dbl>          <dbl>           <dbl>          <dbl>\n##  1   8.1        556       21              1               2              1\n##  2   8.1        479       22              1               2              1\n##  3   8          455       21              1               0              1\n##  4   7.8        426       21              1               2              1\n##  5   7.5        391       21              1               1              1\n##  6   8.4        384       21              1               2              1\n##  7   7.6        358       21              1               1              1\n##  8   8.2        358       21              1               2              1\n##  9   8.1        371       21              1               1              1\n## 10   8          346       21              1               1              1\n## # ... with 359 more rows, and 4 more variables: jeepers <dbl>, jinkies <dbl>,\n## #   my_glasses <dbl>, zoinks <dbl>\n\nback1 <- lm(imdb ~ ., data=scoobydoo)\n\nsummary(back1)## \n## Call:\n## lm(formula = imdb ~ ., data = scoobydoo)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.84731 -0.30812 -0.02295  0.33015  2.48946 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      8.221e+00  7.074e-02 116.212  < 2e-16 ***\n## engagement      -6.610e-06  5.797e-06  -1.140 0.254938    \n## run_time        -1.040e-02  2.293e-03  -4.536 7.84e-06 ***\n## monster_amount  -4.792e-02  2.054e-02  -2.333 0.020207 *  \n## suspects_amount -6.519e-02  1.489e-02  -4.377 1.58e-05 ***\n## culprit_amount   1.304e-02  2.813e-02   0.464 0.643254    \n## jeepers         -2.581e-02  2.384e-02  -1.083 0.279719    \n## jinkies         -5.536e-02  1.573e-02  -3.519 0.000488 ***\n## my_glasses      -1.156e-01  9.429e-02  -1.226 0.221045    \n## zoinks           8.496e-03  1.167e-02   0.728 0.467106    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5547 on 359 degrees of freedom\n## Multiple R-squared:  0.3503, Adjusted R-squared:  0.334 \n## F-statistic: 21.51 on 9 and 359 DF,  p-value: < 2.2e-16"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-1-1","chapter":"4 Lab 7 â€” November 10","heading":"4.5.2 Dropping a predictor, round 1","text":"Remember dropping predictors, related hypothesis test, want fail \nreject null hypothesis. Therefore looking \\(p\\)-values greater \n\\(\\alpha_{\\text{exit}} = 0.05\\). output , many predictors eligible \ndropped. Therefore, look predictor corresponding largest \\(p\\)-value greater \n\\(\\alpha_{\\text{exit}} = 0.05\\). culprit_amount \\(p\\)-value 0.643.","code":"\ndrop1(back1, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## imdb ~ engagement + run_time + monster_amount + suspects_amount + \n##     culprit_amount + jeepers + jinkies + my_glasses + zoinks\n##                 Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n## <none>                       110.48 -425.00                      \n## engagement       1    0.4001 110.88 -425.66  1.3002 0.2549380    \n## run_time         1    6.3308 116.81 -406.43 20.5713 7.843e-06 ***\n## monster_amount   1    1.6748 112.16 -421.44  5.4422 0.0202074 *  \n## suspects_amount  1    5.8966 116.38 -407.81 19.1605 1.578e-05 ***\n## culprit_amount   1    0.0661 110.55 -426.77  0.2149 0.6432544    \n## jeepers          1    0.3607 110.84 -425.79  1.1720 0.2797190    \n## jinkies          1    3.8118 114.29 -414.48 12.3861 0.0004882 ***\n## my_glasses       1    0.4625 110.94 -425.45  1.5028 0.2210448    \n## zoinks           1    0.1631 110.64 -426.45  0.5299 0.4671058    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nback2 <- update(back1, . ~ . - culprit_amount)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-2-1","chapter":"4 Lab 7 â€” November 10","heading":"4.5.3 Dropping a predictor, round 2","text":"variable largest \\(p\\)-value greater \\(\\alpha_{\\text{exit}} = 0.05\\) zoinks.","code":"\ndrop1(back2, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## imdb ~ engagement + run_time + monster_amount + suspects_amount + \n##     jeepers + jinkies + my_glasses + zoinks\n##                 Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n## <none>                       110.55 -426.77                      \n## engagement       1    0.4102 110.96 -427.41  1.3359   0.24853    \n## run_time         1    6.2755 116.82 -408.40 20.4364 8.377e-06 ***\n## monster_amount   1    1.6124 112.16 -423.43  5.2509   0.02251 *  \n## suspects_amount  1    5.8491 116.40 -409.75 19.0476 1.668e-05 ***\n## jeepers          1    0.3504 110.90 -427.61  1.1409   0.28617    \n## jinkies          1    3.8516 114.40 -416.14 12.5429   0.00045 ***\n## my_glasses       1    0.4734 111.02 -427.20  1.5416   0.21518    \n## zoinks           1    0.1479 110.69 -428.28  0.4816   0.48816    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nback3 <- update(back2, . ~ . - zoinks)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-3","chapter":"4 Lab 7 â€” November 10","heading":"4.5.4 Dropping a predictor, round 3","text":"variable largest \\(p\\)-value greater \\(\\alpha_{\\text{exit}} = 0.05\\) jeepers.","code":"\ndrop1(back3, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## imdb ~ engagement + run_time + monster_amount + suspects_amount + \n##     jeepers + jinkies + my_glasses\n##                 Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n## <none>                       110.69 -428.28                      \n## engagement       1    0.3912 111.09 -428.98  1.2758 0.2594306    \n## run_time         1    6.2058 116.90 -410.15 20.2383 9.231e-06 ***\n## monster_amount   1    1.5627 112.26 -425.11  5.0961 0.0245764 *  \n## suspects_amount  1    5.8494 116.55 -411.28 19.0762 1.643e-05 ***\n## jeepers          1    0.2204 110.92 -429.55  0.7188 0.3970933    \n## jinkies          1    3.7181 114.41 -418.09 12.1255 0.0005584 ***\n## my_glasses       1    0.4628 111.16 -428.74  1.5093 0.2200451    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nback4 <- update(back3, . ~ . - jeepers)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-4","chapter":"4 Lab 7 â€” November 10","heading":"4.5.5 Dropping a predictor, round 4","text":"variable largest \\(p\\)-value greater \\(\\alpha_{\\text{exit}} = 0.05\\) engagement.","code":"\ndrop1(back4, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## imdb ~ engagement + run_time + monster_amount + suspects_amount + \n##     jinkies + my_glasses\n##                 Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n## <none>                       110.92 -429.55                      \n## engagement       1    0.3585 111.27 -430.36  1.1702 0.2800874    \n## run_time         1    6.6587 117.57 -410.03 21.7322 4.414e-06 ***\n## monster_amount   1    1.5324 112.45 -426.48  5.0015 0.0259342 *  \n## suspects_amount  1    5.8929 116.81 -412.45 19.2328 1.519e-05 ***\n## jinkies          1    3.7525 114.67 -419.27 12.2471 0.0005241 ***\n## my_glasses       1    0.4743 111.39 -429.97  1.5479 0.2142514    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nback5 <- update(back4, . ~ . - engagement)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-5","chapter":"4 Lab 7 â€” November 10","heading":"4.5.6 Dropping a predictor, round 5","text":"variable largest \\(p\\)-value greater \\(\\alpha_{\\text{exit}} = 0.05\\) my_glasses.","code":"\ndrop1(back5, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## imdb ~ run_time + monster_amount + suspects_amount + jinkies + \n##     my_glasses\n##                 Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n## <none>                       111.27 -430.36                      \n## run_time         1    7.2140 118.49 -409.18 23.5337 1.825e-06 ***\n## monster_amount   1    2.3265 113.60 -424.72  7.5896 0.0061662 ** \n## suspects_amount  1    6.0337 117.31 -412.87 19.6833 1.214e-05 ***\n## jinkies          1    4.1226 115.40 -418.93 13.4489 0.0002818 ***\n## my_glasses       1    0.8231 112.10 -429.64  2.6852 0.1021524    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nback6 <- update(back5, . ~ . - my_glasses)"},{"path":"lab-7-november-10.html","id":"dropping-a-predictor-round-6","chapter":"4 Lab 7 â€” November 10","heading":"4.5.7 Dropping a predictor, round 6","text":"remaining single term deletions \\(p\\)-values less \n\\(\\alpha_{\\text{exit}} = 0.05\\). Therefore, stop .","code":"\ndrop1(back6, ~ ., test=\"F\")## Single term deletions\n## \n## Model:\n## imdb ~ run_time + monster_amount + suspects_amount + jinkies\n##                 Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n## <none>                       112.10 -429.64                      \n## run_time         1    8.2939 120.39 -405.30 26.9316  3.51e-07 ***\n## monster_amount   1    2.4078 114.50 -423.79  7.8187 0.0054452 ** \n## suspects_amount  1    5.9769 118.07 -412.47 19.4080  1.39e-05 ***\n## jinkies          1    4.3332 116.43 -417.64 14.0708 0.0002048 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lab-7-november-10.html","id":"the-final-model-1","chapter":"4 Lab 7 â€” November 10","heading":"4.5.8 The final model","text":"final model :\\[\\widehat{Y}_{} \\,=\\, 8.2645 \\,-\\, 0.0114X_{1,\\,} \\,-\\, 0.0524X_{2,\\,} - 0.0647X_{3,\\,} - 0.0579X_{4,\\,}\\]Interestingly, says number jinkies episode negatively impact IMDB\nscore ðŸ˜‚!Note also, remaining predictors model significant, \npoor adjusted R-squared value 0.3336. means explained 33.36% \nvariation IMDB ratings using predictors. , model still room \nimprovement (though keep mind small subset variables included \nexample sake brevity).","code":"\nsummary(back6)## \n## Call:\n## lm(formula = imdb ~ run_time + monster_amount + suspects_amount + \n##     jinkies, data = scoobydoo)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.89720 -0.30625 -0.03038  0.33928  2.42649 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      8.264472   0.062373 132.501  < 2e-16 ***\n## run_time        -0.011483   0.002213  -5.190 3.51e-07 ***\n## monster_amount  -0.052395   0.018738  -2.796 0.005445 ** \n## suspects_amount -0.064694   0.014685  -4.405 1.39e-05 ***\n## jinkies         -0.057869   0.015427  -3.751 0.000205 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5549 on 364 degrees of freedom\n## Multiple R-squared:  0.3408, Adjusted R-squared:  0.3336 \n## F-statistic: 47.04 on 4 and 364 DF,  p-value: < 2.2e-16"}]
